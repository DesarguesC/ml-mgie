{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d90bd3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# For licensing see accompanying LICENSE file.\n",
    "# Copyright (C) 2024 Apple Inc. All Rights Reserved.\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "39465df4-3ac7-4340-b71d-02c11c78d1be",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, pickle, io, base64, json\n",
    "\n",
    "from glob import glob\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "from PIL import Image\n",
    "\n",
    "import torch as T\n",
    "import transformers\n",
    "\n",
    "from llava.conversation import conv_templates\n",
    "from llava.model import *\n",
    "\n",
    "def f2b(f):\n",
    "    b = io.BytesIO()\n",
    "    f.save(b, format='JPEG')\n",
    "    b = str(base64.b64encode(b.getvalue()))[2:-1]\n",
    "    return b\n",
    "def b2f(b):\n",
    "    return Image.open(io.BytesIO(base64.b64decode(b))).convert('RGB')\n",
    "def crop_resize(f, sz=512):\n",
    "    w, h = f.size\n",
    "    if w>h:\n",
    "        p = (w-h)//2\n",
    "        f = f.crop([p, 0, p+h, h])\n",
    "    elif h>w:\n",
    "        p = (h-w)//2\n",
    "        f = f.crop([0, p, w, p+w])\n",
    "    f = f.resize([sz, sz])\n",
    "    return f\n",
    "def remove_alter(s):  # hack expressive instruction\n",
    "    if 'ASSISTANT:' in s: s = s[s.index('ASSISTANT:')+10:].strip()\n",
    "    if '</s>' in s: s = s[:s.index('</s>')].strip()\n",
    "    if 'alternative' in s.lower(): s = s[:s.lower().index('alternative')]\n",
    "    if '[IMG0]' in s: s = s[:s.index('[IMG0]')]\n",
    "    s = '.'.join([s.strip() for s in s.split('.')[:2]])\n",
    "    if s[-1]!='.': s += '.'\n",
    "    return s.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7004c5ab-7ccc-4914-b1d9-0314ebfd5a6f",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at ../autodl-tmp/openai/clip-vit-large-patch14 were not used when initializing CLIPVisionModel: ['text_model.encoder.layers.11.self_attn.k_proj.weight', 'text_model.encoder.layers.1.self_attn.v_proj.bias', 'text_model.encoder.layers.4.self_attn.v_proj.weight', 'text_model.encoder.layers.3.layer_norm2.weight', 'text_model.encoder.layers.1.self_attn.k_proj.bias', 'text_model.encoder.layers.0.layer_norm2.bias', 'text_model.encoder.layers.4.mlp.fc2.weight', 'text_model.encoder.layers.3.layer_norm1.weight', 'text_model.encoder.layers.10.self_attn.v_proj.weight', 'text_model.encoder.layers.5.self_attn.v_proj.weight', 'text_model.encoder.layers.7.layer_norm2.bias', 'text_model.encoder.layers.8.mlp.fc2.bias', 'text_model.encoder.layers.7.self_attn.k_proj.weight', 'visual_projection.weight', 'text_model.encoder.layers.0.self_attn.v_proj.weight', 'text_model.encoder.layers.1.self_attn.k_proj.weight', 'text_model.encoder.layers.2.mlp.fc1.weight', 'text_model.encoder.layers.9.mlp.fc2.bias', 'text_model.encoder.layers.6.self_attn.q_proj.weight', 'text_model.encoder.layers.3.self_attn.k_proj.bias', 'text_model.encoder.layers.9.self_attn.q_proj.bias', 'text_model.encoder.layers.11.self_attn.q_proj.bias', 'text_model.encoder.layers.0.self_attn.v_proj.bias', 'text_model.encoder.layers.0.layer_norm2.weight', 'text_model.encoder.layers.3.mlp.fc2.weight', 'text_model.encoder.layers.11.mlp.fc2.bias', 'text_model.encoder.layers.9.mlp.fc2.weight', 'text_model.encoder.layers.7.self_attn.q_proj.bias', 'text_model.encoder.layers.3.self_attn.v_proj.weight', 'text_model.encoder.layers.6.layer_norm2.bias', 'text_model.encoder.layers.7.mlp.fc1.weight', 'text_model.encoder.layers.4.self_attn.v_proj.bias', 'text_model.encoder.layers.11.layer_norm1.bias', 'text_model.encoder.layers.0.self_attn.out_proj.bias', 'text_model.encoder.layers.9.self_attn.out_proj.bias', 'text_model.encoder.layers.10.layer_norm2.weight', 'text_model.encoder.layers.5.layer_norm2.bias', 'text_model.encoder.layers.6.layer_norm2.weight', 'text_model.encoder.layers.11.self_attn.k_proj.bias', 'text_model.encoder.layers.6.mlp.fc2.weight', 'text_model.encoder.layers.4.layer_norm1.weight', 'text_model.encoder.layers.0.mlp.fc1.weight', 'text_model.encoder.layers.10.self_attn.q_proj.weight', 'text_model.encoder.layers.10.layer_norm2.bias', 'text_model.encoder.layers.9.layer_norm2.bias', 'text_model.encoder.layers.3.self_attn.v_proj.bias', 'text_model.encoder.layers.5.self_attn.k_proj.bias', 'text_model.encoder.layers.8.self_attn.v_proj.weight', 'text_model.encoder.layers.2.layer_norm2.weight', 'text_model.encoder.layers.6.self_attn.v_proj.weight', 'text_model.embeddings.position_embedding.weight', 'text_model.encoder.layers.8.layer_norm1.weight', 'text_model.encoder.layers.8.mlp.fc2.weight', 'text_model.encoder.layers.1.self_attn.out_proj.weight', 'text_model.encoder.layers.3.self_attn.out_proj.bias', 'text_model.encoder.layers.11.self_attn.out_proj.weight', 'text_projection.weight', 'text_model.encoder.layers.2.mlp.fc2.bias', 'text_model.encoder.layers.2.self_attn.k_proj.bias', 'text_model.encoder.layers.10.mlp.fc2.bias', 'text_model.encoder.layers.2.self_attn.v_proj.bias', 'text_model.encoder.layers.3.mlp.fc2.bias', 'text_model.encoder.layers.5.self_attn.out_proj.weight', 'text_model.encoder.layers.10.layer_norm1.weight', 'text_model.encoder.layers.5.self_attn.q_proj.weight', 'text_model.encoder.layers.5.layer_norm2.weight', 'text_model.encoder.layers.5.layer_norm1.weight', 'text_model.encoder.layers.10.self_attn.out_proj.weight', 'text_model.encoder.layers.11.layer_norm2.weight', 'text_model.encoder.layers.5.self_attn.v_proj.bias', 'text_model.encoder.layers.0.mlp.fc2.weight', 'text_model.encoder.layers.9.self_attn.k_proj.weight', 'text_model.encoder.layers.4.self_attn.out_proj.weight', 'text_model.encoder.layers.0.layer_norm1.weight', 'text_model.encoder.layers.10.self_attn.k_proj.weight', 'text_model.encoder.layers.5.mlp.fc2.weight', 'text_model.encoder.layers.6.mlp.fc1.weight', 'text_model.encoder.layers.2.self_attn.q_proj.weight', 'text_model.encoder.layers.7.layer_norm1.bias', 'text_model.encoder.layers.2.self_attn.out_proj.bias', 'text_model.encoder.layers.1.self_attn.q_proj.bias', 'text_model.encoder.layers.10.mlp.fc1.weight', 'text_model.encoder.layers.3.layer_norm2.bias', 'text_model.encoder.layers.10.mlp.fc2.weight', 'text_model.encoder.layers.8.self_attn.v_proj.bias', 'text_model.encoder.layers.3.mlp.fc1.weight', 'text_model.encoder.layers.3.self_attn.out_proj.weight', 'text_model.encoder.layers.3.self_attn.k_proj.weight', 'text_model.encoder.layers.8.self_attn.q_proj.bias', 'text_model.encoder.layers.8.layer_norm2.bias', 'text_model.encoder.layers.4.mlp.fc1.weight', 'text_model.encoder.layers.8.self_attn.q_proj.weight', 'text_model.encoder.layers.0.mlp.fc2.bias', 'text_model.encoder.layers.10.self_attn.k_proj.bias', 'text_model.encoder.layers.4.layer_norm2.weight', 'text_model.encoder.layers.4.layer_norm2.bias', 'text_model.encoder.layers.10.self_attn.v_proj.bias', 'text_model.encoder.layers.0.self_attn.q_proj.bias', 'text_model.encoder.layers.11.layer_norm2.bias', 'text_model.encoder.layers.4.mlp.fc2.bias', 'text_model.encoder.layers.8.layer_norm1.bias', 'text_model.encoder.layers.7.layer_norm1.weight', 'text_model.encoder.layers.11.self_attn.v_proj.bias', 'text_model.encoder.layers.1.mlp.fc1.weight', 'text_model.encoder.layers.9.self_attn.v_proj.bias', 'text_model.encoder.layers.11.mlp.fc1.bias', 'text_model.encoder.layers.0.self_attn.q_proj.weight', 'text_model.encoder.layers.2.self_attn.out_proj.weight', 'text_model.encoder.layers.1.layer_norm1.bias', 'text_model.encoder.layers.11.self_attn.v_proj.weight', 'text_model.encoder.layers.7.self_attn.q_proj.weight', 'text_model.final_layer_norm.bias', 'text_model.encoder.layers.1.self_attn.q_proj.weight', 'text_model.encoder.layers.7.self_attn.v_proj.weight', 'logit_scale', 'text_model.encoder.layers.7.mlp.fc2.bias', 'text_model.encoder.layers.5.mlp.fc2.bias', 'text_model.encoder.layers.0.self_attn.k_proj.bias', 'text_model.encoder.layers.2.mlp.fc1.bias', 'text_model.encoder.layers.11.mlp.fc2.weight', 'text_model.encoder.layers.10.mlp.fc1.bias', 'text_model.encoder.layers.1.layer_norm2.bias', 'text_model.encoder.layers.3.mlp.fc1.bias', 'text_model.encoder.layers.7.self_attn.out_proj.bias', 'text_model.encoder.layers.7.mlp.fc1.bias', 'text_model.encoder.layers.0.self_attn.k_proj.weight', 'text_model.encoder.layers.5.mlp.fc1.bias', 'text_model.encoder.layers.6.self_attn.q_proj.bias', 'text_model.encoder.layers.6.self_attn.k_proj.bias', 'text_model.encoder.layers.8.layer_norm2.weight', 'text_model.encoder.layers.4.self_attn.out_proj.bias', 'text_model.encoder.layers.4.mlp.fc1.bias', 'text_model.encoder.layers.9.mlp.fc1.weight', 'text_model.encoder.layers.1.mlp.fc1.bias', 'text_model.encoder.layers.2.mlp.fc2.weight', 'text_model.encoder.layers.6.self_attn.out_proj.bias', 'text_model.encoder.layers.5.self_attn.q_proj.bias', 'text_model.encoder.layers.6.mlp.fc2.bias', 'text_model.encoder.layers.0.mlp.fc1.bias', 'text_model.encoder.layers.5.self_attn.k_proj.weight', 'text_model.encoder.layers.2.self_attn.v_proj.weight', 'text_model.encoder.layers.0.layer_norm1.bias', 'text_model.encoder.layers.5.mlp.fc1.weight', 'text_model.encoder.layers.6.self_attn.k_proj.weight', 'text_model.encoder.layers.9.layer_norm1.weight', 'text_model.encoder.layers.1.self_attn.v_proj.weight', 'text_model.encoder.layers.10.self_attn.q_proj.bias', 'text_model.encoder.layers.6.self_attn.out_proj.weight', 'text_model.encoder.layers.1.mlp.fc2.weight', 'text_model.encoder.layers.0.self_attn.out_proj.weight', 'text_model.encoder.layers.9.layer_norm1.bias', 'text_model.encoder.layers.6.mlp.fc1.bias', 'text_model.encoder.layers.4.self_attn.q_proj.bias', 'text_model.embeddings.position_ids', 'text_model.encoder.layers.1.layer_norm2.weight', 'text_model.encoder.layers.4.self_attn.q_proj.weight', 'text_model.encoder.layers.3.self_attn.q_proj.bias', 'text_model.encoder.layers.8.self_attn.out_proj.weight', 'text_model.encoder.layers.8.mlp.fc1.bias', 'text_model.encoder.layers.5.self_attn.out_proj.bias', 'text_model.encoder.layers.6.layer_norm1.weight', 'text_model.encoder.layers.2.layer_norm1.bias', 'text_model.encoder.layers.10.self_attn.out_proj.bias', 'text_model.encoder.layers.11.self_attn.q_proj.weight', 'text_model.encoder.layers.11.self_attn.out_proj.bias', 'text_model.encoder.layers.1.layer_norm1.weight', 'text_model.encoder.layers.1.self_attn.out_proj.bias', 'text_model.encoder.layers.7.self_attn.out_proj.weight', 'text_model.encoder.layers.10.layer_norm1.bias', 'text_model.embeddings.token_embedding.weight', 'text_model.encoder.layers.5.layer_norm1.bias', 'text_model.encoder.layers.4.layer_norm1.bias', 'text_model.encoder.layers.7.layer_norm2.weight', 'text_model.encoder.layers.9.mlp.fc1.bias', 'text_model.encoder.layers.1.mlp.fc2.bias', 'text_model.encoder.layers.2.self_attn.q_proj.bias', 'text_model.encoder.layers.9.layer_norm2.weight', 'text_model.encoder.layers.4.self_attn.k_proj.weight', 'text_model.final_layer_norm.weight', 'text_model.encoder.layers.2.layer_norm1.weight', 'text_model.encoder.layers.9.self_attn.out_proj.weight', 'text_model.encoder.layers.9.self_attn.v_proj.weight', 'text_model.encoder.layers.6.layer_norm1.bias', 'text_model.encoder.layers.2.layer_norm2.bias', 'text_model.encoder.layers.9.self_attn.q_proj.weight', 'text_model.encoder.layers.6.self_attn.v_proj.bias', 'text_model.encoder.layers.11.layer_norm1.weight', 'text_model.encoder.layers.3.layer_norm1.bias', 'text_model.encoder.layers.9.self_attn.k_proj.bias', 'text_model.encoder.layers.8.self_attn.k_proj.weight', 'text_model.encoder.layers.8.self_attn.k_proj.bias', 'text_model.encoder.layers.4.self_attn.k_proj.bias', 'text_model.encoder.layers.7.self_attn.v_proj.bias', 'text_model.encoder.layers.3.self_attn.q_proj.weight', 'text_model.encoder.layers.8.mlp.fc1.weight', 'text_model.encoder.layers.8.self_attn.out_proj.bias', 'text_model.encoder.layers.2.self_attn.k_proj.weight', 'text_model.encoder.layers.7.mlp.fc2.weight', 'text_model.encoder.layers.7.self_attn.k_proj.bias', 'text_model.encoder.layers.11.mlp.fc1.weight']\n",
      "- This IS expected if you are initializing CLIPVisionModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing CLIPVisionModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a859b1d70ee149628ceca8ee748ca8d6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of LlavaLlamaForCausalLM were not initialized from the model checkpoint at ../autodl-tmp/LLaVA-7B-v1 and are newly initialized: ['unet.down_blocks.2.attentions.0.transformer_blocks.0.attn2.to_v.weight', 'vae.decoder.up_blocks.3.resnets.1.conv2.weight', 'unet.down_blocks.1.attentions.0.proj_in.weight', 'vae.decoder.up_blocks.1.resnets.1.norm2.bias', 'unet.up_blocks.2.attentions.1.transformer_blocks.0.norm3.weight', 'unet.mid_block.resnets.1.time_emb_proj.bias', 'unet.down_blocks.2.attentions.0.transformer_blocks.0.norm3.bias', 'vae.decoder.mid_block.resnets.1.norm1.bias', 'unet.up_blocks.1.attentions.1.transformer_blocks.0.attn2.to_v.weight', 'unet.up_blocks.1.attentions.2.transformer_blocks.0.attn2.to_k.weight', 'unet.down_blocks.3.resnets.1.conv2.weight', 'unet.mid_block.resnets.0.time_emb_proj.bias', 'edit_head.mapper.decoder.layers.2.norm1.weight', 'unet.down_blocks.1.resnets.0.conv2.weight', 'unet.up_blocks.2.attentions.0.transformer_blocks.0.ff.net.2.bias', 'vae.decoder.up_blocks.0.resnets.1.conv2.weight', 'vae.encoder.down_blocks.1.resnets.0.conv1.weight', 'unet.down_blocks.3.resnets.1.norm2.bias', 'unet.up_blocks.1.attentions.1.transformer_blocks.0.norm1.bias', 'unet.up_blocks.1.resnets.2.conv2.weight', 'vae.decoder.up_blocks.1.resnets.0.norm1.bias', 'edit_head.mapper.decoder.layers.1.linear1.bias', 'unet.up_blocks.0.resnets.2.conv1.weight', 'unet.down_blocks.0.resnets.1.norm1.bias', 'unet.up_blocks.1.attentions.1.transformer_blocks.0.attn2.to_out.0.weight', 'vae.encoder.down_blocks.0.resnets.0.conv1.weight', 'vae.encoder.conv_norm_out.bias', 'vae.encoder.conv_norm_out.weight', 'edit_head.mapper.decoder.layers.2.norm2.weight', 'edit_head.mapper.decoder.layers.2.multihead_attn.out_proj.weight', 'unet.down_blocks.1.attentions.1.proj_in.bias', 'vae.encoder.down_blocks.0.resnets.0.conv2.weight', 'unet.down_blocks.1.resnets.0.norm1.weight', 'unet.up_blocks.2.attentions.0.transformer_blocks.0.norm2.weight', 'unet.up_blocks.3.attentions.0.transformer_blocks.0.attn1.to_q.weight', 'vae.encoder.down_blocks.2.resnets.1.conv1.bias', 'unet.mid_block.resnets.0.norm2.bias', 'unet.up_blocks.1.attentions.2.transformer_blocks.0.attn2.to_v.weight', 'vae.decoder.up_blocks.2.resnets.0.norm2.bias', 'unet.down_blocks.2.attentions.0.transformer_blocks.0.attn1.to_k.weight', 'edit_head.mapper.encoder.layers.1.norm1.bias', 'edit_head.mapper.encoder.layers.3.norm2.weight', 'vae.decoder.mid_block.attentions.0.group_norm.weight', 'unet.down_blocks.0.downsamplers.0.conv.weight', 'unet.up_blocks.1.resnets.2.norm2.weight', 'unet.down_blocks.2.attentions.0.proj_in.weight', 'unet.mid_block.attentions.0.transformer_blocks.0.norm3.bias', 'edit_head.mapper.encoder.layers.3.linear1.weight', 'vae.decoder.up_blocks.1.resnets.2.conv2.weight', 'unet.down_blocks.0.attentions.0.transformer_blocks.0.attn1.to_q.weight', 'vae.decoder.up_blocks.1.resnets.1.conv1.weight', 'unet.up_blocks.1.attentions.0.transformer_blocks.0.norm2.weight', 'edit_head.mapper.decoder.layers.3.self_attn.out_proj.bias', 'unet.down_blocks.1.attentions.1.transformer_blocks.0.norm1.bias', 'vae.decoder.mid_block.resnets.0.norm2.weight', 'unet.down_blocks.3.resnets.1.time_emb_proj.weight', 'unet.up_blocks.3.attentions.2.transformer_blocks.0.ff.net.0.proj.bias', 'vae.decoder.up_blocks.0.resnets.1.norm1.weight', 'unet.down_blocks.0.attentions.1.transformer_blocks.0.attn1.to_k.weight', 'unet.down_blocks.2.attentions.1.proj_in.weight', 'unet.up_blocks.2.attentions.0.transformer_blocks.0.attn2.to_out.0.weight', 'unet.down_blocks.0.resnets.0.conv1.bias', 'vae.decoder.up_blocks.0.resnets.0.norm2.weight', 'unet.down_blocks.2.attentions.1.transformer_blocks.0.attn1.to_v.weight', 'unet.up_blocks.3.attentions.1.transformer_blocks.0.norm1.bias', 'edit_head.mapper.encoder.norm.weight', 'unet.down_blocks.1.attentions.0.transformer_blocks.0.attn2.to_v.weight', 'unet.up_blocks.2.attentions.0.transformer_blocks.0.attn2.to_out.0.bias', 'unet.up_blocks.3.attentions.2.transformer_blocks.0.ff.net.2.bias', 'vae.decoder.up_blocks.0.resnets.2.norm2.bias', 'unet.down_blocks.0.attentions.0.proj_in.bias', 'edit_head.mapper.encoder.layers.3.norm1.weight', 'vae.encoder.down_blocks.0.downsamplers.0.conv.bias', 'unet.down_blocks.2.resnets.0.conv1.bias', 'unet.up_blocks.1.resnets.2.norm1.bias', 'vae.decoder.conv_in.weight', 'unet.up_blocks.2.resnets.2.norm1.weight', 'unet.up_blocks.3.attentions.0.transformer_blocks.0.attn1.to_out.0.bias', 'unet.down_blocks.1.resnets.1.conv2.weight', 'unet.up_blocks.3.attentions.0.transformer_blocks.0.norm2.weight', 'unet.up_blocks.3.attentions.1.transformer_blocks.0.attn1.to_v.weight', 'unet.mid_block.resnets.1.conv1.weight', 'vae.encoder.mid_block.resnets.0.norm2.weight', 'edit_head.mapper.decoder.layers.3.multihead_attn.out_proj.weight', 'unet.up_blocks.1.attentions.0.transformer_blocks.0.norm1.weight', 'unet.up_blocks.1.resnets.0.time_emb_proj.bias', 'unet.up_blocks.3.attentions.1.transformer_blocks.0.attn1.to_k.weight', 'vae.decoder.mid_block.resnets.1.conv2.weight', 'vae.decoder.up_blocks.3.resnets.0.norm2.weight', 'edit_head.mapper.decoder.layers.2.norm3.weight', 'unet.down_blocks.2.attentions.0.transformer_blocks.0.attn2.to_out.0.weight', 'vae.decoder.up_blocks.0.resnets.0.conv1.bias', 'edit_head.mapper.decoder.layers.3.linear2.bias', 'vae.decoder.up_blocks.2.resnets.0.conv_shortcut.weight', 'vae.encoder.mid_block.resnets.1.conv1.weight', 'unet.up_blocks.0.resnets.2.norm2.bias', 'unet.up_blocks.2.attentions.2.proj_out.weight', 'unet.mid_block.attentions.0.transformer_blocks.0.ff.net.0.proj.weight', 'vae.decoder.conv_out.bias', 'unet.up_blocks.2.attentions.0.transformer_blocks.0.attn1.to_out.0.bias', 'edit_head.mapper.decoder.layers.1.norm2.bias', 'edit_head.mapper.encoder.layers.2.linear1.bias', 'edit_head.mapper.decoder.layers.0.self_attn.in_proj_bias', 'edit_head.mapper.encoder.layers.0.self_attn.in_proj_bias', 'unet.up_blocks.1.resnets.0.conv1.bias', 'unet.up_blocks.3.attentions.1.transformer_blocks.0.ff.net.2.weight', 'unet.down_blocks.0.downsamplers.0.conv.bias', 'unet.up_blocks.3.attentions.2.norm.bias', 'vae.encoder.down_blocks.2.resnets.0.conv1.bias', 'vae.decoder.up_blocks.2.resnets.2.norm2.weight', 'vae.decoder.up_blocks.2.resnets.1.norm1.weight', 'vae.decoder.up_blocks.3.resnets.2.norm1.weight', 'unet.up_blocks.3.attentions.2.transformer_blocks.0.attn2.to_v.weight', 'edit_head.mapper.decoder.layers.1.linear1.weight', 'unet.down_blocks.2.attentions.0.transformer_blocks.0.norm1.bias', 'vae.encoder.mid_block.resnets.1.conv1.bias', 'unet.down_blocks.3.resnets.0.norm1.bias', 'unet.down_blocks.3.resnets.0.conv1.weight', 'unet.up_blocks.3.resnets.1.time_emb_proj.weight', 'unet.up_blocks.0.upsamplers.0.conv.bias', 'unet.down_blocks.0.resnets.0.norm1.weight', 'vae.encoder.down_blocks.0.resnets.1.norm1.bias', 'unet.down_blocks.1.attentions.0.norm.weight', 'unet.up_blocks.0.resnets.1.norm1.bias', 'unet.down_blocks.1.resnets.1.norm1.bias', 'unet.down_blocks.2.attentions.0.proj_out.weight', 'unet.up_blocks.1.upsamplers.0.conv.bias', 'unet.up_blocks.3.attentions.1.transformer_blocks.0.norm1.weight', 'vae.decoder.mid_block.attentions.0.to_q.weight', 'edit_head.mapper.decoder.layers.2.linear2.weight', 'edit_head.mapper.decoder.layers.1.self_attn.in_proj_weight', 'vae.decoder.up_blocks.1.resnets.1.norm1.weight', 'edit_head.mapper.decoder.layers.1.norm3.weight', 'unet.down_blocks.1.attentions.0.transformer_blocks.0.attn1.to_out.0.bias', 'unet.up_blocks.2.attentions.1.norm.bias', 'unet.up_blocks.2.resnets.0.conv2.weight', 'unet.up_blocks.3.attentions.1.transformer_blocks.0.attn2.to_q.weight', 'vae.encoder.down_blocks.0.resnets.1.conv1.weight', 'edit_head.query', 'unet.down_blocks.1.resnets.0.norm1.bias', 'unet.up_blocks.0.resnets.1.norm2.weight', 'unet.up_blocks.1.attentions.2.proj_in.weight', 'unet.mid_block.attentions.0.transformer_blocks.0.attn1.to_v.weight', 'vae.encoder.mid_block.resnets.0.norm2.bias', 'unet.down_blocks.0.attentions.1.proj_out.weight', 'unet.up_blocks.3.attentions.2.proj_in.bias', 'vae.decoder.up_blocks.3.resnets.1.conv1.weight', 'unet.down_blocks.3.resnets.1.norm1.bias', 'edit_head.mapper.encoder.layers.2.self_attn.out_proj.bias', 'unet.up_blocks.0.resnets.2.norm2.weight', 'unet.down_blocks.0.attentions.1.transformer_blocks.0.norm3.weight', 'unet.up_blocks.2.resnets.2.norm2.bias', 'unet.up_blocks.1.attentions.2.proj_out.bias', 'unet.up_blocks.2.attentions.2.transformer_blocks.0.attn2.to_out.0.bias', 'edit_head.mapper.decoder.layers.2.norm1.bias', 'unet.up_blocks.1.attentions.2.transformer_blocks.0.attn2.to_q.weight', 'unet.up_blocks.1.resnets.0.conv2.weight', 'unet.up_blocks.2.attentions.1.transformer_blocks.0.attn2.to_k.weight', 'unet.up_blocks.2.attentions.0.transformer_blocks.0.norm2.bias', 'unet.up_blocks.1.attentions.1.transformer_blocks.0.ff.net.0.proj.weight', 'edit_head.mapper.decoder.layers.0.multihead_attn.out_proj.weight', 'unet.down_blocks.2.attentions.0.transformer_blocks.0.attn1.to_out.0.bias', 'unet.down_blocks.2.resnets.0.conv_shortcut.weight', 'unet.down_blocks.2.resnets.1.conv2.bias', 'edit_head.mapper.decoder.layers.0.multihead_attn.out_proj.bias', 'vae.encoder.mid_block.attentions.0.to_k.weight', 'unet.down_blocks.2.attentions.1.transformer_blocks.0.attn1.to_q.weight', 'unet.up_blocks.1.resnets.0.norm1.weight', 'unet.up_blocks.1.resnets.2.norm2.bias', 'edit_head.mapper.decoder.layers.0.linear2.weight', 'edit_head.mapper.encoder.layers.1.norm2.bias', 'unet.up_blocks.2.attentions.0.proj_in.weight', 'unet.down_blocks.0.resnets.0.conv1.weight', 'vae.encoder.down_blocks.3.resnets.0.conv1.weight', 'vae.decoder.mid_block.attentions.0.to_k.bias', 'unet.down_blocks.2.resnets.1.norm1.weight', 'unet.down_blocks.2.resnets.0.time_emb_proj.weight', 'unet.down_blocks.0.attentions.0.transformer_blocks.0.attn2.to_v.weight', 'unet.up_blocks.0.resnets.2.conv_shortcut.weight', 'unet.up_blocks.1.attentions.1.transformer_blocks.0.norm3.weight', 'vae.decoder.up_blocks.1.resnets.1.conv2.bias', 'unet.down_blocks.2.attentions.1.transformer_blocks.0.attn2.to_k.weight', 'edit_head.mapper.decoder.layers.0.norm2.bias', 'unet.down_blocks.0.attentions.0.transformer_blocks.0.attn1.to_k.weight', 'edit_head.mapper.decoder.layers.1.linear2.weight', 'unet.down_blocks.0.attentions.1.transformer_blocks.0.attn1.to_out.0.weight', 'unet.down_blocks.1.attentions.0.transformer_blocks.0.norm3.weight', 'unet.up_blocks.3.resnets.2.norm2.bias', 'unet.mid_block.attentions.0.transformer_blocks.0.attn1.to_out.0.bias', 'unet.up_blocks.2.attentions.1.transformer_blocks.0.attn2.to_out.0.weight', 'unet.down_blocks.2.downsamplers.0.conv.bias', 'unet.up_blocks.2.resnets.0.norm1.bias', 'edit_head.mapper.encoder.layers.1.norm1.weight', 'unet.mid_block.resnets.0.conv1.bias', 'unet.up_blocks.1.attentions.0.transformer_blocks.0.attn1.to_out.0.bias', 'unet.up_blocks.1.attentions.2.transformer_blocks.0.attn2.to_out.0.weight', 'unet.mid_block.resnets.0.norm1.bias', 'unet.down_blocks.1.attentions.1.transformer_blocks.0.attn1.to_k.weight', 'unet.down_blocks.2.resnets.0.conv1.weight', 'unet.up_blocks.3.attentions.1.proj_out.weight', 'edit_head.mapper.encoder.layers.1.norm2.weight', 'unet.down_blocks.1.resnets.1.norm1.weight', 'edit_head.mapper.encoder.layers.0.norm1.bias', 'vae.encoder.down_blocks.2.resnets.1.norm2.bias', 'edit_head.hid2feat.weight', 'unet.down_blocks.3.resnets.0.time_emb_proj.bias', 'vae.decoder.up_blocks.3.resnets.2.conv1.bias', 'vae.decoder.mid_block.resnets.0.norm2.bias', 'edit_head.mapper.decoder.layers.3.norm2.weight', 'unet.up_blocks.1.resnets.2.conv_shortcut.weight', 'unet.up_blocks.3.attentions.1.transformer_blocks.0.attn1.to_out.0.weight', 'unet.down_blocks.1.resnets.0.time_emb_proj.weight', 'unet.up_blocks.3.resnets.0.time_emb_proj.weight', 'vae.decoder.up_blocks.3.resnets.2.conv1.weight', 'unet.up_blocks.1.attentions.1.transformer_blocks.0.attn2.to_out.0.bias', 'unet.mid_block.resnets.0.norm1.weight', 'unet.down_blocks.2.attentions.1.transformer_blocks.0.attn1.to_k.weight', 'unet.down_blocks.2.attentions.0.transformer_blocks.0.attn2.to_out.0.bias', 'unet.down_blocks.2.attentions.0.transformer_blocks.0.attn1.to_out.0.weight', 'vae.encoder.down_blocks.2.resnets.1.norm1.bias', 'edit_head.mapper.decoder.layers.1.multihead_attn.in_proj_weight', 'vae.encoder.mid_block.resnets.1.norm1.bias', 'unet.up_blocks.1.attentions.1.transformer_blocks.0.attn1.to_out.0.bias', 'unet.down_blocks.0.attentions.0.transformer_blocks.0.attn2.to_out.0.bias', 'unet.down_blocks.2.downsamplers.0.conv.weight', 'unet.up_blocks.2.attentions.0.transformer_blocks.0.attn2.to_k.weight', 'vae.encoder.down_blocks.2.resnets.0.conv1.weight', 'unet.up_blocks.1.attentions.0.transformer_blocks.0.attn1.to_k.weight', 'vae.encoder.mid_block.resnets.1.conv2.bias', 'unet.up_blocks.1.attentions.2.transformer_blocks.0.ff.net.2.weight', 'unet.down_blocks.2.attentions.1.transformer_blocks.0.norm1.weight', 'vae.encoder.down_blocks.0.resnets.0.conv2.bias', 'unet.time_embedding.linear_1.weight', 'unet.up_blocks.0.resnets.0.norm1.weight', 'vae.encoder.down_blocks.2.resnets.0.norm1.weight', 'vae.decoder.up_blocks.2.resnets.2.conv2.bias', 'unet.up_blocks.1.attentions.1.transformer_blocks.0.attn1.to_out.0.weight', 'unet.mid_block.resnets.1.conv2.weight', 'unet.down_blocks.1.attentions.1.transformer_blocks.0.attn2.to_out.0.bias', 'unet.up_blocks.3.resnets.2.conv_shortcut.weight', 'unet.down_blocks.2.resnets.0.conv2.weight', 'unet.up_blocks.2.attentions.0.transformer_blocks.0.attn1.to_out.0.weight', 'unet.up_blocks.1.resnets.2.time_emb_proj.weight', 'vae.decoder.up_blocks.0.resnets.0.conv2.weight', 'unet.conv_norm_out.weight', 'unet.down_blocks.0.attentions.0.transformer_blocks.0.attn1.to_v.weight', 'unet.up_blocks.0.resnets.2.time_emb_proj.weight', 'unet.up_blocks.3.attentions.0.transformer_blocks.0.attn2.to_v.weight', 'unet.down_blocks.2.attentions.1.transformer_blocks.0.attn2.to_q.weight', 'unet.up_blocks.2.resnets.0.norm1.weight', 'vae.decoder.mid_block.attentions.0.to_k.weight', 'unet.up_blocks.3.attentions.2.transformer_blocks.0.norm2.weight', 'unet.down_blocks.0.attentions.0.transformer_blocks.0.ff.net.0.proj.weight', 'unet.down_blocks.1.attentions.0.proj_out.weight', 'vae.encoder.down_blocks.1.resnets.0.conv2.weight', 'unet.down_blocks.2.attentions.1.transformer_blocks.0.attn1.to_out.0.bias', 'unet.down_blocks.2.attentions.1.transformer_blocks.0.ff.net.2.bias', 'unet.up_blocks.2.attentions.2.transformer_blocks.0.norm3.weight', 'vae.encoder.down_blocks.0.resnets.1.norm2.weight', 'unet.up_blocks.2.resnets.2.conv2.bias', 'vae.encoder.down_blocks.1.resnets.0.norm2.bias', 'unet.mid_block.attentions.0.transformer_blocks.0.attn2.to_k.weight', 'unet.up_blocks.2.upsamplers.0.conv.bias', 'unet.down_blocks.2.resnets.0.conv_shortcut.bias', 'unet.up_blocks.3.attentions.0.transformer_blocks.0.attn2.to_out.0.bias', 'unet.up_blocks.2.resnets.2.conv_shortcut.weight', 'vae.encoder.down_blocks.3.resnets.1.norm2.bias', 'unet.up_blocks.3.resnets.0.norm1.weight', 'edit_head.mapper.decoder.layers.2.self_attn.in_proj_weight', 'vae.encoder.down_blocks.1.resnets.1.norm2.bias', 'unet.mid_block.resnets.1.norm1.weight', 'unet.mid_block.attentions.0.transformer_blocks.0.attn1.to_k.weight', 'unet.down_blocks.1.attentions.1.transformer_blocks.0.norm3.bias', 'unet.down_blocks.1.attentions.1.transformer_blocks.0.ff.net.0.proj.weight', 'edit_head.mapper.encoder.layers.3.self_attn.in_proj_bias', 'edit_head.mapper.encoder.layers.2.linear1.weight', 'edit_head.mapper.encoder.layers.1.linear2.weight', 'vae.decoder.up_blocks.1.resnets.1.norm1.bias', 'edit_head.mapper.decoder.layers.1.multihead_attn.in_proj_bias', 'unet.up_blocks.0.resnets.0.conv2.weight', 'vae.encoder.down_blocks.3.resnets.0.norm1.weight', 'unet.up_blocks.3.attentions.0.proj_out.bias', 'unet.mid_block.resnets.0.norm2.weight', 'unet.down_blocks.0.resnets.1.conv1.weight', 'vae.decoder.up_blocks.2.resnets.1.conv1.weight', 'unet.up_blocks.3.resnets.2.norm1.weight', 'unet.down_blocks.1.attentions.1.transformer_blocks.0.attn2.to_out.0.weight', 'unet.up_blocks.1.attentions.0.transformer_blocks.0.ff.net.0.proj.bias', 'unet.up_blocks.2.attentions.2.proj_in.bias', 'unet.mid_block.attentions.0.transformer_blocks.0.ff.net.2.bias', 'vae.encoder.down_blocks.3.resnets.0.norm2.weight', 'vae.decoder.mid_block.resnets.1.conv2.bias', 'unet.down_blocks.0.attentions.0.norm.bias', 'unet.up_blocks.2.resnets.1.conv_shortcut.bias', 'unet.up_blocks.1.resnets.1.conv2.bias', 'unet.up_blocks.2.resnets.2.conv1.weight', 'unet.up_blocks.3.attentions.1.transformer_blocks.0.attn2.to_v.weight', 'unet.up_blocks.3.resnets.1.norm2.bias', 'unet.up_blocks.1.resnets.1.norm2.bias', 'unet.down_blocks.3.resnets.0.time_emb_proj.weight', 'unet.up_blocks.3.resnets.0.conv1.bias', 'unet.down_blocks.1.attentions.1.transformer_blocks.0.norm3.weight', 'unet.mid_block.resnets.1.conv1.bias', 'edit_head.mapper.encoder.layers.2.self_attn.in_proj_weight', 'unet.down_blocks.2.attentions.1.transformer_blocks.0.norm2.weight', 'unet.down_blocks.2.attentions.1.proj_out.bias', 'edit_head.mapper.decoder.layers.0.multihead_attn.in_proj_weight', 'unet.up_blocks.2.attentions.0.transformer_blocks.0.attn2.to_v.weight', 'edit_head.mapper.decoder.layers.0.norm1.weight', 'unet.down_blocks.1.attentions.0.transformer_blocks.0.ff.net.0.proj.weight', 'vae.encoder.down_blocks.2.resnets.1.conv2.bias', 'vae.encoder.mid_block.attentions.0.to_k.bias', 'vae.encoder.mid_block.attentions.0.to_q.weight', 'unet.up_blocks.1.attentions.1.proj_out.weight', 'unet.up_blocks.3.attentions.0.transformer_blocks.0.ff.net.2.weight', 'unet.up_blocks.1.attentions.0.transformer_blocks.0.attn1.to_out.0.weight', 'unet.up_blocks.1.resnets.1.time_emb_proj.bias', 'unet.mid_block.attentions.0.transformer_blocks.0.ff.net.2.weight', 'vae.encoder.down_blocks.3.resnets.1.conv1.bias', 'unet.time_embedding.linear_2.weight', 'unet.up_blocks.2.resnets.0.norm2.bias', 'vae.decoder.mid_block.resnets.0.conv2.weight', 'vae.decoder.up_blocks.3.resnets.0.conv1.weight', 'unet.up_blocks.3.resnets.0.conv_shortcut.bias', 'unet.up_blocks.3.attentions.0.transformer_blocks.0.attn1.to_k.weight', 'unet.up_blocks.1.attentions.0.norm.bias', 'unet.up_blocks.2.attentions.0.transformer_blocks.0.attn1.to_k.weight', 'unet.mid_block.attentions.0.transformer_blocks.0.norm1.weight', 'unet.up_blocks.2.attentions.2.proj_out.bias', 'vae.decoder.conv_norm_out.bias', 'vae.decoder.mid_block.resnets.1.norm2.weight', 'unet.up_blocks.1.attentions.2.transformer_blocks.0.norm3.weight', 'unet.up_blocks.1.attentions.1.norm.bias', 'unet.up_blocks.1.attentions.0.norm.weight', 'unet.down_blocks.0.attentions.1.transformer_blocks.0.norm2.bias', 'unet.down_blocks.2.resnets.0.norm2.weight', 'unet.up_blocks.1.attentions.1.transformer_blocks.0.norm1.weight', 'unet.up_blocks.1.attentions.0.proj_out.bias', 'unet.down_blocks.1.attentions.0.transformer_blocks.0.attn2.to_out.0.weight', 'unet.up_blocks.3.resnets.1.conv_shortcut.weight', 'unet.up_blocks.1.attentions.0.transformer_blocks.0.norm1.bias', 'unet.up_blocks.3.resnets.0.time_emb_proj.bias', 'vae.decoder.mid_block.resnets.0.norm1.bias', 'unet.up_blocks.2.resnets.0.time_emb_proj.weight', 'vae.decoder.up_blocks.0.resnets.2.conv2.bias', 'unet.down_blocks.2.attentions.0.transformer_blocks.0.norm2.weight', 'unet.up_blocks.2.resnets.1.conv2.bias', 'unet.down_blocks.0.attentions.0.transformer_blocks.0.norm3.weight', 'unet.up_blocks.3.resnets.2.time_emb_proj.weight', 'vae.decoder.conv_out.weight', 'unet.down_blocks.3.resnets.0.conv2.weight', 'edit_head.mapper.decoder.layers.0.norm2.weight', 'unet.down_blocks.1.attentions.0.transformer_blocks.0.ff.net.0.proj.bias', 'vae.encoder.down_blocks.3.resnets.1.norm2.weight', 'vae.decoder.up_blocks.2.resnets.0.conv1.bias', 'unet.up_blocks.3.attentions.1.transformer_blocks.0.ff.net.0.proj.weight', 'unet.time_embedding.linear_2.bias', 'unet.up_blocks.1.resnets.2.conv1.bias', 'vae.encoder.conv_in.weight', 'vae.decoder.up_blocks.0.resnets.2.norm1.bias', 'vae.decoder.up_blocks.3.resnets.2.conv2.weight', 'unet.up_blocks.3.attentions.0.transformer_blocks.0.norm3.weight', 'vae.decoder.up_blocks.2.resnets.2.conv1.bias', 'unet.up_blocks.2.resnets.1.time_emb_proj.weight', 'unet.down_blocks.0.attentions.1.transformer_blocks.0.attn1.to_v.weight', 'unet.up_blocks.3.attentions.1.proj_in.weight', 'unet.up_blocks.0.resnets.1.conv_shortcut.weight', 'edit_head.mapper.decoder.norm.bias', 'unet.down_blocks.0.resnets.0.conv2.weight', 'unet.up_blocks.2.attentions.2.transformer_blocks.0.attn1.to_out.0.weight', 'unet.up_blocks.2.resnets.1.conv1.weight', 'edit_head.mapper.encoder.layers.1.self_attn.out_proj.bias', 'vae.decoder.up_blocks.3.resnets.1.norm2.weight', 'unet.up_blocks.2.attentions.1.transformer_blocks.0.ff.net.0.proj.bias', 'unet.down_blocks.1.attentions.1.proj_out.bias', 'unet.up_blocks.3.attentions.2.transformer_blocks.0.attn1.to_out.0.bias', 'unet.up_blocks.2.attentions.2.transformer_blocks.0.norm3.bias', 'unet.up_blocks.2.attentions.2.norm.weight', 'unet.up_blocks.2.attentions.0.transformer_blocks.0.norm1.weight', 'unet.up_blocks.2.resnets.1.time_emb_proj.bias', 'unet.down_blocks.0.attentions.1.transformer_blocks.0.attn2.to_out.0.weight', 'unet.down_blocks.2.attentions.1.transformer_blocks.0.attn2.to_out.0.bias', 'edit_head.mapper.encoder.layers.0.linear2.weight', 'unet.down_blocks.0.attentions.0.transformer_blocks.0.ff.net.2.weight', 'edit_head.mapper.decoder.layers.0.linear2.bias', 'edit_head.mapper.decoder.layers.2.linear1.weight', 'unet.up_blocks.2.attentions.1.proj_out.weight', 'unet.up_blocks.0.resnets.1.conv1.bias', 'unet.up_blocks.0.resnets.1.norm2.bias', 'vae.decoder.up_blocks.3.resnets.1.norm1.weight', 'unet.up_blocks.2.resnets.0.conv_shortcut.bias', 'vae.encoder.down_blocks.1.resnets.0.norm1.weight', 'vae.decoder.conv_in.bias', 'unet.down_blocks.1.attentions.0.transformer_blocks.0.ff.net.2.weight', 'unet.up_blocks.3.attentions.0.transformer_blocks.0.norm1.weight', 'vae.decoder.up_blocks.1.resnets.1.conv2.weight', 'vae.encoder.down_blocks.1.resnets.1.conv1.weight', 'unet.up_blocks.1.attentions.1.transformer_blocks.0.attn1.to_k.weight', 'unet.down_blocks.1.attentions.0.transformer_blocks.0.attn2.to_q.weight', 'vae.encoder.mid_block.attentions.0.to_out.0.bias', 'vae.decoder.up_blocks.1.resnets.1.norm2.weight', 'edit_head.mapper.decoder.layers.1.norm1.weight', 'vae.decoder.mid_block.resnets.0.conv1.weight', 'unet.up_blocks.1.resnets.1.norm1.weight', 'unet.up_blocks.2.resnets.1.norm1.weight', 'unet.down_blocks.1.attentions.1.transformer_blocks.0.attn2.to_q.weight', 'unet.up_blocks.3.resnets.1.norm2.weight', 'vae.encoder.down_blocks.0.resnets.1.conv2.bias', 'unet.down_blocks.2.attentions.1.transformer_blocks.0.norm3.weight', 'vae.decoder.up_blocks.1.resnets.2.norm2.weight', 'unet.down_blocks.0.resnets.0.time_emb_proj.bias', 'vae.decoder.mid_block.resnets.1.conv1.weight', 'unet.up_blocks.3.resnets.0.norm1.bias', 'unet.down_blocks.0.attentions.0.transformer_blocks.0.norm1.weight', 'vae.decoder.up_blocks.1.resnets.2.conv1.bias', 'unet.up_blocks.2.resnets.1.conv2.weight', 'unet.down_blocks.0.resnets.0.norm2.bias', 'vae.decoder.up_blocks.0.resnets.1.norm2.bias', 'unet.down_blocks.3.resnets.0.norm1.weight', 'unet.up_blocks.0.resnets.0.conv1.bias', 'unet.up_blocks.1.resnets.0.norm2.weight', 'edit_head.mapper.decoder.layers.0.norm1.bias', 'unet.up_blocks.1.attentions.2.transformer_blocks.0.norm1.bias', 'vae.encoder.down_blocks.0.downsamplers.0.conv.weight', 'unet.up_blocks.3.resnets.1.conv1.bias', 'edit_head.mapper.decoder.layers.1.multihead_attn.out_proj.bias', 'unet.mid_block.attentions.0.transformer_blocks.0.attn2.to_v.weight', 'unet.up_blocks.3.resnets.2.conv2.weight', 'edit_head.mapper.decoder.layers.0.multihead_attn.in_proj_bias', 'unet.up_blocks.2.attentions.2.transformer_blocks.0.attn2.to_k.weight', 'unet.up_blocks.1.resnets.2.conv_shortcut.bias', 'unet.up_blocks.3.attentions.0.transformer_blocks.0.norm1.bias', 'unet.up_blocks.3.resnets.1.conv2.bias', 'unet.up_blocks.1.attentions.0.transformer_blocks.0.norm3.bias', 'vae.decoder.mid_block.resnets.0.norm1.weight', 'unet.down_blocks.1.resnets.0.conv_shortcut.weight', 'vae.decoder.up_blocks.0.resnets.0.norm1.bias', 'vae.encoder.mid_block.resnets.0.norm1.weight', 'unet.up_blocks.1.attentions.1.norm.weight', 'unet.up_blocks.2.attentions.0.proj_out.bias', 'vae.encoder.mid_block.resnets.0.conv2.weight', 'vae.encoder.down_blocks.3.resnets.1.conv2.bias', 'unet.up_blocks.3.attentions.0.proj_in.weight', 'edit_head.llm2hid.weight', 'unet.up_blocks.1.attentions.2.proj_out.weight', 'edit_head.mapper.decoder.layers.1.multihead_attn.out_proj.weight', 'unet.up_blocks.1.attentions.0.transformer_blocks.0.attn1.to_v.weight', 'vae.decoder.mid_block.resnets.0.conv1.bias', 'unet.up_blocks.0.resnets.0.conv1.weight', 'unet.down_blocks.1.attentions.0.transformer_blocks.0.attn1.to_q.weight', 'unet.up_blocks.2.attentions.0.norm.weight', 'unet.down_blocks.1.attentions.0.transformer_blocks.0.attn1.to_k.weight', 'edit_head.llm2hid.bias', 'vae.encoder.down_blocks.1.downsamplers.0.conv.bias', 'vae.decoder.mid_block.attentions.0.to_v.bias', 'unet.up_blocks.1.resnets.0.time_emb_proj.weight', 'unet.down_blocks.1.resnets.1.time_emb_proj.bias', 'unet.down_blocks.0.attentions.1.transformer_blocks.0.norm2.weight', 'unet.down_blocks.2.attentions.0.transformer_blocks.0.attn1.to_q.weight', 'unet.up_blocks.2.resnets.2.conv2.weight', 'unet.down_blocks.1.downsamplers.0.conv.weight', 'vae.decoder.up_blocks.2.resnets.0.norm2.weight', 'unet.up_blocks.3.attentions.1.transformer_blocks.0.ff.net.0.proj.bias', 'unet.up_blocks.1.attentions.1.transformer_blocks.0.norm2.weight', 'vae.decoder.up_blocks.0.resnets.1.norm2.weight', 'unet.up_blocks.3.attentions.2.transformer_blocks.0.norm1.bias', 'edit_head.mapper.decoder.layers.0.self_attn.in_proj_weight', 'vae.decoder.up_blocks.0.resnets.1.norm1.bias', 'vae.quant_conv.bias', 'unet.down_blocks.1.resnets.0.conv1.bias', 'unet.down_blocks.0.resnets.1.conv1.bias', 'unet.up_blocks.0.resnets.1.time_emb_proj.bias', 'unet.up_blocks.3.attentions.2.transformer_blocks.0.attn2.to_out.0.bias', 'unet.down_blocks.0.attentions.1.transformer_blocks.0.attn2.to_k.weight', 'unet.conv_in.bias', 'unet.down_blocks.2.resnets.1.conv2.weight', 'unet.up_blocks.3.resnets.0.norm2.weight', 'unet.up_blocks.2.attentions.1.transformer_blocks.0.norm3.bias', 'unet.down_blocks.2.attentions.0.transformer_blocks.0.attn2.to_q.weight', 'unet.down_blocks.0.attentions.1.proj_in.weight', 'unet.down_blocks.0.resnets.0.time_emb_proj.weight', 'unet.up_blocks.0.resnets.1.conv_shortcut.bias', 'edit_head.mapper.encoder.layers.2.linear2.weight', 'unet.up_blocks.2.resnets.0.conv1.bias', 'vae.encoder.mid_block.attentions.0.to_q.bias', 'unet.up_blocks.1.resnets.2.time_emb_proj.bias', 'unet.up_blocks.2.attentions.2.transformer_blocks.0.attn1.to_v.weight', 'edit_head.mapper.encoder.layers.3.self_attn.out_proj.bias', 'vae.encoder.down_blocks.3.resnets.0.conv2.bias', 'unet.up_blocks.1.attentions.0.transformer_blocks.0.ff.net.0.proj.weight', 'unet.up_blocks.3.resnets.2.conv2.bias', 'unet.up_blocks.3.attentions.2.transformer_blocks.0.attn1.to_q.weight', 'vae.decoder.up_blocks.1.resnets.2.conv2.bias', 'unet.up_blocks.1.attentions.0.proj_in.weight', 'vae.decoder.mid_block.attentions.0.group_norm.bias', 'unet.conv_in.weight', 'vae.post_quant_conv.weight', 'edit_head.mapper.decoder.layers.3.linear2.weight', 'unet.up_blocks.1.attentions.2.transformer_blocks.0.attn2.to_out.0.bias', 'vae.decoder.up_blocks.2.resnets.0.norm1.bias', 'unet.down_blocks.1.attentions.1.transformer_blocks.0.attn2.to_v.weight', 'edit_head.mapper.decoder.layers.1.linear2.bias', 'unet.up_blocks.2.attentions.2.transformer_blocks.0.attn2.to_v.weight', 'unet.up_blocks.1.attentions.0.transformer_blocks.0.norm3.weight', 'vae.decoder.up_blocks.3.resnets.1.conv1.bias', 'unet.down_blocks.0.attentions.1.proj_out.bias', 'unet.up_blocks.0.resnets.0.norm2.weight', 'vae.decoder.up_blocks.3.resnets.0.conv_shortcut.bias', 'edit_head.mapper.encoder.layers.1.self_attn.in_proj_weight', 'unet.down_blocks.2.attentions.1.proj_out.weight', 'vae.decoder.up_blocks.0.resnets.1.conv2.bias', 'unet.up_blocks.2.attentions.2.transformer_blocks.0.ff.net.0.proj.bias', 'vae.decoder.up_blocks.3.resnets.0.conv2.weight', 'unet.up_blocks.3.attentions.2.transformer_blocks.0.attn2.to_out.0.weight', 'vae.decoder.up_blocks.0.upsamplers.0.conv.weight', 'unet.down_blocks.0.resnets.0.conv2.bias', 'edit_head.mapper.encoder.layers.1.linear1.bias', 'unet.down_blocks.0.attentions.0.proj_out.bias', 'unet.down_blocks.2.attentions.1.transformer_blocks.0.attn1.to_out.0.weight', 'vae.decoder.up_blocks.2.resnets.1.norm2.weight', 'unet.up_blocks.1.attentions.2.transformer_blocks.0.ff.net.0.proj.bias', 'vae.decoder.up_blocks.2.resnets.1.conv2.weight', 'unet.conv_out.bias', 'vae.encoder.down_blocks.2.downsamplers.0.conv.weight', 'unet.down_blocks.1.attentions.1.transformer_blocks.0.ff.net.2.bias', 'vae.decoder.up_blocks.1.upsamplers.0.conv.bias', 'unet.up_blocks.1.resnets.1.norm2.weight', 'unet.up_blocks.2.resnets.2.conv1.bias', 'edit_head.mapper.encoder.layers.0.self_attn.out_proj.bias', 'unet.up_blocks.3.attentions.1.norm.bias', 'unet.down_blocks.0.attentions.1.transformer_blocks.0.ff.net.2.bias', 'unet.down_blocks.2.resnets.0.norm1.bias', 'unet.down_blocks.1.resnets.0.conv_shortcut.bias', 'vae.decoder.mid_block.attentions.0.to_v.weight', 'unet.up_blocks.2.attentions.0.transformer_blocks.0.attn2.to_q.weight', 'unet.up_blocks.3.attentions.0.transformer_blocks.0.ff.net.2.bias', 'edit_head.mapper.encoder.layers.1.linear1.weight', 'vae.encoder.down_blocks.0.resnets.1.conv1.bias', 'edit_head.mapper.encoder.layers.2.norm2.weight', 'unet.down_blocks.2.resnets.1.conv1.weight', 'unet.down_blocks.0.attentions.0.transformer_blocks.0.ff.net.2.bias', 'unet.up_blocks.1.attentions.1.transformer_blocks.0.norm3.bias', 'vae.encoder.down_blocks.2.resnets.1.norm2.weight', 'edit_head.mapper.decoder.layers.3.norm2.bias', 'vae.decoder.up_blocks.2.resnets.0.conv_shortcut.bias', 'vae.decoder.up_blocks.3.resnets.2.norm2.bias', 'vae.decoder.up_blocks.3.resnets.0.norm2.bias', 'unet.mid_block.resnets.1.norm2.bias', 'unet.up_blocks.3.attentions.1.transformer_blocks.0.attn2.to_out.0.weight', 'vae.encoder.down_blocks.2.downsamplers.0.conv.bias', 'unet.down_blocks.0.attentions.1.transformer_blocks.0.ff.net.0.proj.bias', 'edit_head.mapper.decoder.layers.3.self_attn.out_proj.weight', 'unet.up_blocks.2.attentions.2.transformer_blocks.0.attn2.to_out.0.weight', 'edit_head.mapper.encoder.layers.0.norm2.weight', 'unet.up_blocks.0.resnets.2.time_emb_proj.bias', 'edit_head.mapper.encoder.layers.0.linear2.bias', 'edit_head.mapper.decoder.layers.2.multihead_attn.out_proj.bias', 'unet.down_blocks.1.resnets.1.conv1.weight', 'vae.encoder.down_blocks.0.resnets.1.norm1.weight', 'vae.encoder.down_blocks.2.resnets.0.conv_shortcut.weight', 'edit_head.mapper.encoder.layers.0.norm1.weight', 'vae.decoder.up_blocks.2.resnets.2.norm1.weight', 'unet.up_blocks.1.attentions.2.transformer_blocks.0.attn1.to_k.weight', 'edit_head.mapper.decoder.layers.1.norm3.bias', 'unet.up_blocks.3.attentions.1.transformer_blocks.0.norm2.bias', 'unet.down_blocks.3.resnets.1.time_emb_proj.bias', 'unet.up_blocks.3.resnets.1.conv1.weight', 'unet.up_blocks.2.attentions.2.transformer_blocks.0.norm1.weight', 'unet.up_blocks.2.attentions.0.transformer_blocks.0.norm3.weight', 'unet.up_blocks.2.attentions.1.transformer_blocks.0.norm1.weight', 'edit_head.mapper.decoder.layers.0.self_attn.out_proj.weight', 'vae.decoder.up_blocks.0.upsamplers.0.conv.bias', 'vae.decoder.up_blocks.0.resnets.2.norm1.weight', 'unet.down_blocks.1.downsamplers.0.conv.bias', 'unet.down_blocks.3.resnets.1.conv1.bias', 'unet.down_blocks.1.attentions.0.transformer_blocks.0.attn2.to_out.0.bias', 'vae.encoder.down_blocks.1.resnets.0.conv2.bias', 'edit_head.mapper.encoder.layers.2.linear2.bias', 'edit_head.mapper.decoder.layers.3.norm3.weight', 'unet.up_blocks.3.resnets.0.conv2.bias', 'unet.up_blocks.3.attentions.1.proj_in.bias', 'unet.down_blocks.0.attentions.1.transformer_blocks.0.attn2.to_v.weight', 'vae.encoder.down_blocks.3.resnets.0.norm2.bias', 'unet.up_blocks.1.attentions.0.proj_out.weight', 'unet.up_blocks.2.resnets.1.norm2.weight', 'unet.conv_out.weight', 'unet.up_blocks.1.attentions.0.transformer_blocks.0.attn2.to_v.weight', 'unet.up_blocks.3.attentions.0.proj_out.weight', 'vae.encoder.down_blocks.0.resnets.0.norm1.weight', 'unet.down_blocks.1.attentions.0.norm.bias', 'unet.down_blocks.2.resnets.1.time_emb_proj.weight', 'vae.decoder.mid_block.attentions.0.to_q.bias', 'vae.encoder.down_blocks.1.resnets.1.norm1.weight', 'unet.down_blocks.2.attentions.0.transformer_blocks.0.attn1.to_v.weight', 'unet.down_blocks.3.resnets.0.norm2.weight', 'vae.decoder.up_blocks.2.resnets.0.conv1.weight', 'unet.down_blocks.0.attentions.0.transformer_blocks.0.attn2.to_q.weight', 'unet.down_blocks.1.attentions.1.transformer_blocks.0.ff.net.2.weight', 'vae.encoder.down_blocks.2.resnets.1.conv1.weight', 'unet.down_blocks.0.attentions.0.transformer_blocks.0.norm2.weight', 'unet.up_blocks.1.attentions.2.transformer_blocks.0.norm1.weight', 'vae.decoder.up_blocks.3.resnets.0.conv_shortcut.weight', 'unet.up_blocks.2.attentions.1.proj_in.bias', 'unet.up_blocks.3.resnets.0.norm2.bias', 'edit_head.mapper.encoder.layers.0.self_attn.in_proj_weight', 'vae.encoder.conv_out.bias', 'unet.up_blocks.2.resnets.2.norm2.weight', 'vae.encoder.down_blocks.0.resnets.1.norm2.bias', 'unet.up_blocks.2.attentions.2.proj_in.weight', 'unet.up_blocks.2.resnets.2.time_emb_proj.weight', 'edit_head.mapper.decoder.layers.3.norm1.weight', 'unet.down_blocks.0.resnets.1.conv2.bias', 'vae.encoder.down_blocks.1.resnets.1.norm2.weight', 'unet.up_blocks.0.resnets.2.conv_shortcut.bias', 'edit_head.mapper.encoder.layers.2.norm2.bias', 'unet.up_blocks.0.resnets.0.time_emb_proj.weight', 'unet.up_blocks.3.attentions.0.norm.bias', 'unet.up_blocks.2.attentions.0.transformer_blocks.0.norm1.bias', 'unet.up_blocks.3.resnets.1.time_emb_proj.bias', 'unet.down_blocks.1.attentions.0.transformer_blocks.0.norm2.bias', 'vae.encoder.mid_block.attentions.0.to_v.weight', 'unet.down_blocks.3.resnets.1.conv2.bias', 'edit_head.mapper.decoder.layers.3.norm1.bias', 'unet.down_blocks.2.resnets.1.norm2.weight', 'unet.up_blocks.1.attentions.2.transformer_blocks.0.attn1.to_q.weight', 'unet.up_blocks.2.attentions.1.transformer_blocks.0.attn2.to_out.0.bias', 'unet.down_blocks.1.resnets.0.norm2.weight', 'unet.up_blocks.3.resnets.1.conv2.weight', 'unet.down_blocks.0.attentions.1.proj_in.bias', 'unet.up_blocks.0.upsamplers.0.conv.weight', 'vae.encoder.down_blocks.2.resnets.1.conv2.weight', 'unet.up_blocks.0.resnets.0.conv_shortcut.weight', 'unet.up_blocks.3.attentions.2.transformer_blocks.0.attn1.to_v.weight', 'vae.decoder.mid_block.resnets.1.norm2.bias', 'edit_head.mapper.decoder.layers.0.norm3.weight', 'unet.down_blocks.0.attentions.1.transformer_blocks.0.attn2.to_q.weight', 'unet.down_blocks.2.attentions.0.proj_out.bias', 'unet.up_blocks.0.resnets.0.conv_shortcut.bias', 'unet.up_blocks.3.attentions.0.transformer_blocks.0.ff.net.0.proj.weight', 'unet.down_blocks.0.resnets.1.norm1.weight', 'edit_head.mapper.encoder.layers.3.linear2.weight', 'unet.up_blocks.2.attentions.1.transformer_blocks.0.norm2.bias', 'unet.up_blocks.1.attentions.0.transformer_blocks.0.ff.net.2.bias', 'unet.up_blocks.2.attentions.1.transformer_blocks.0.attn1.to_v.weight', 'vae.quant_conv.weight', 'unet.up_blocks.3.attentions.2.transformer_blocks.0.attn1.to_out.0.weight', 'vae.encoder.down_blocks.2.resnets.0.norm2.weight', 'vae.decoder.up_blocks.1.resnets.2.conv1.weight', 'unet.mid_block.attentions.0.transformer_blocks.0.attn2.to_out.0.weight', 'vae.decoder.up_blocks.0.resnets.0.conv2.bias', 'unet.up_blocks.1.resnets.0.conv2.bias', 'unet.down_blocks.0.attentions.0.transformer_blocks.0.attn1.to_out.0.bias', 'unet.up_blocks.1.resnets.1.time_emb_proj.weight', 'edit_head.mapper.decoder.layers.2.linear2.bias', 'unet.mid_block.resnets.0.conv1.weight', 'vae.decoder.up_blocks.1.resnets.0.conv2.weight', 'unet.up_blocks.3.attentions.1.transformer_blocks.0.attn1.to_q.weight', 'vae.encoder.down_blocks.1.resnets.1.conv2.weight', 'vae.decoder.mid_block.resnets.0.conv2.bias', 'unet.down_blocks.2.attentions.1.transformer_blocks.0.attn2.to_v.weight', 'vae.encoder.down_blocks.0.resnets.0.norm2.weight', 'vae.decoder.up_blocks.0.resnets.1.conv1.weight', 'edit_head.mapper.encoder.layers.1.linear2.bias', 'unet.up_blocks.1.attentions.1.proj_in.weight', 'unet.up_blocks.1.attentions.2.transformer_blocks.0.attn1.to_v.weight', 'vae.decoder.up_blocks.1.resnets.0.norm2.weight', 'unet.up_blocks.2.resnets.0.conv2.bias', 'unet.up_blocks.1.attentions.1.transformer_blocks.0.attn2.to_k.weight', 'vae.decoder.mid_block.attentions.0.to_out.0.bias', 'unet.down_blocks.1.attentions.1.proj_in.weight', 'vae.decoder.up_blocks.3.resnets.0.conv2.bias', 'unet.up_blocks.2.resnets.1.conv_shortcut.weight', 'unet.up_blocks.1.attentions.0.transformer_blocks.0.attn2.to_out.0.weight', 'unet.up_blocks.1.resnets.2.conv1.weight', 'edit_head.mapper.decoder.layers.1.self_attn.out_proj.bias', 'unet.mid_block.attentions.0.transformer_blocks.0.attn1.to_q.weight', 'unet.up_blocks.1.resnets.0.conv1.weight', 'unet.down_blocks.0.attentions.1.transformer_blocks.0.attn2.to_out.0.bias', 'vae.decoder.up_blocks.1.resnets.0.norm1.weight', 'unet.up_blocks.1.upsamplers.0.conv.weight', 'edit_head.mapper.decoder.layers.3.linear1.weight', 'unet.up_blocks.1.resnets.2.conv2.bias', 'unet.up_blocks.3.resnets.1.norm1.bias', 'edit_head.mapper.encoder.norm.bias', 'unet.up_blocks.3.attentions.2.transformer_blocks.0.attn2.to_q.weight', 'unet.up_blocks.3.attentions.2.proj_out.weight', 'unet.conv_norm_out.bias', 'vae.encoder.mid_block.attentions.0.to_v.bias', 'vae.decoder.up_blocks.2.resnets.2.norm2.bias', 'unet.up_blocks.0.resnets.1.time_emb_proj.weight', 'unet.up_blocks.3.attentions.0.transformer_blocks.0.norm2.bias', 'unet.up_blocks.2.attentions.2.transformer_blocks.0.attn2.to_q.weight', 'unet.mid_block.attentions.0.transformer_blocks.0.norm3.weight', 'vae.encoder.down_blocks.3.resnets.1.norm1.weight', 'unet.up_blocks.0.resnets.2.norm1.weight', 'unet.down_blocks.1.attentions.0.proj_in.bias', 'unet.up_blocks.1.attentions.2.transformer_blocks.0.attn1.to_out.0.weight', 'unet.down_blocks.0.resnets.1.norm2.weight', 'vae.encoder.mid_block.resnets.1.norm2.weight', 'unet.up_blocks.3.resnets.2.norm1.bias', 'unet.up_blocks.3.attentions.2.transformer_blocks.0.ff.net.2.weight', 'unet.up_blocks.1.resnets.1.conv1.weight', 'unet.down_blocks.1.resnets.1.norm2.bias', 'unet.up_blocks.1.resnets.0.norm2.bias', 'edit_head.mapper.encoder.layers.0.norm2.bias', 'edit_head.mapper.encoder.layers.1.self_attn.in_proj_bias', 'unet.up_blocks.3.resnets.1.norm1.weight', 'unet.up_blocks.3.attentions.2.proj_out.bias', 'unet.up_blocks.3.resnets.1.conv_shortcut.bias', 'unet.up_blocks.3.attentions.2.transformer_blocks.0.attn1.to_k.weight', 'unet.up_blocks.3.attentions.0.transformer_blocks.0.ff.net.0.proj.bias', 'vae.encoder.down_blocks.2.resnets.0.conv2.bias', 'unet.down_blocks.1.attentions.0.transformer_blocks.0.ff.net.2.bias', 'unet.up_blocks.3.attentions.0.transformer_blocks.0.attn2.to_out.0.weight', 'unet.up_blocks.2.attentions.1.transformer_blocks.0.attn1.to_k.weight', 'unet.up_blocks.3.attentions.0.norm.weight', 'vae.decoder.up_blocks.2.resnets.1.conv1.bias', 'vae.decoder.up_blocks.1.resnets.2.norm2.bias', 'unet.mid_block.attentions.0.proj_in.bias', 'unet.down_blocks.1.attentions.1.transformer_blocks.0.attn2.to_k.weight', 'unet.up_blocks.2.attentions.1.proj_in.weight', 'unet.up_blocks.1.resnets.1.conv2.weight', 'vae.encoder.down_blocks.0.resnets.0.norm1.bias', 'vae.decoder.up_blocks.3.resnets.1.norm2.bias', 'vae.decoder.up_blocks.3.resnets.2.conv2.bias', 'vae.encoder.down_blocks.1.resnets.0.conv_shortcut.bias', 'vae.encoder.down_blocks.1.resnets.1.norm1.bias', 'vae.encoder.mid_block.resnets.0.norm1.bias', 'unet.mid_block.attentions.0.norm.weight', 'unet.down_blocks.2.attentions.0.transformer_blocks.0.ff.net.0.proj.weight', 'unet.up_blocks.2.resnets.1.norm1.bias', 'unet.down_blocks.3.resnets.0.conv2.bias', 'unet.up_blocks.1.attentions.1.transformer_blocks.0.ff.net.0.proj.bias', 'edit_head.mapper.decoder.layers.0.linear1.bias', 'unet.mid_block.resnets.0.time_emb_proj.weight', 'unet.up_blocks.1.attentions.0.transformer_blocks.0.norm2.bias', 'unet.down_blocks.1.attentions.1.transformer_blocks.0.ff.net.0.proj.bias', 'unet.down_blocks.2.resnets.1.time_emb_proj.bias', 'unet.up_blocks.2.attentions.0.transformer_blocks.0.ff.net.0.proj.weight', 'unet.up_blocks.1.attentions.2.transformer_blocks.0.attn1.to_out.0.bias', 'unet.mid_block.resnets.0.conv2.bias', 'vae.decoder.up_blocks.0.resnets.0.conv1.weight', 'unet.up_blocks.1.attentions.2.norm.weight', 'unet.down_blocks.1.attentions.1.proj_out.weight', 'unet.up_blocks.3.attentions.0.transformer_blocks.0.attn1.to_out.0.weight', 'vae.encoder.down_blocks.3.resnets.1.norm1.bias', 'edit_head.mapper.decoder.layers.3.self_attn.in_proj_weight', 'vae.decoder.up_blocks.2.resnets.1.conv2.bias', 'unet.down_blocks.1.attentions.0.transformer_blocks.0.attn1.to_out.0.weight', 'unet.down_blocks.0.attentions.0.transformer_blocks.0.attn2.to_out.0.weight', 'edit_head.mapper.decoder.layers.1.norm2.weight', 'unet.up_blocks.2.attentions.2.transformer_blocks.0.attn1.to_q.weight', 'unet.up_blocks.0.resnets.0.time_emb_proj.bias', 'unet.up_blocks.3.attentions.2.norm.weight', 'unet.down_blocks.2.attentions.1.norm.bias', 'unet.down_blocks.2.resnets.0.conv2.bias', 'unet.down_blocks.0.attentions.1.transformer_blocks.0.attn1.to_out.0.bias', 'unet.down_blocks.0.resnets.0.norm2.weight', 'unet.up_blocks.3.attentions.1.proj_out.bias', 'vae.decoder.up_blocks.3.resnets.1.conv2.bias', 'vae.encoder.down_blocks.1.resnets.1.conv1.bias', 'vae.encoder.mid_block.attentions.0.to_out.0.weight', 'edit_head.hid2feat.bias', 'unet.up_blocks.3.resnets.0.conv_shortcut.weight', 'unet.up_blocks.1.attentions.0.transformer_blocks.0.attn1.to_q.weight', 'unet.down_blocks.1.attentions.0.transformer_blocks.0.norm1.weight', 'vae.decoder.up_blocks.3.resnets.2.norm2.weight', 'unet.up_blocks.2.attentions.2.transformer_blocks.0.norm2.weight', 'unet.down_blocks.2.resnets.0.time_emb_proj.bias', 'vae.decoder.up_blocks.1.resnets.0.norm2.bias', 'edit_head.mapper.encoder.layers.3.norm1.bias', 'unet.down_blocks.0.attentions.0.transformer_blocks.0.attn2.to_k.weight', 'unet.up_blocks.2.upsamplers.0.conv.weight', 'edit_head.mapper.decoder.layers.3.self_attn.in_proj_bias', 'unet.up_blocks.1.attentions.1.transformer_blocks.0.attn1.to_v.weight', 'vae.decoder.up_blocks.0.resnets.1.conv1.bias', 'unet.down_blocks.0.attentions.1.transformer_blocks.0.norm1.bias', 'vae.decoder.up_blocks.1.resnets.0.conv1.bias', 'unet.down_blocks.1.attentions.0.transformer_blocks.0.norm1.bias', 'edit_head.mapper.decoder.layers.1.self_attn.out_proj.weight', 'unet.down_blocks.1.resnets.1.conv2.bias', 'unet.mid_block.attentions.0.proj_out.weight', 'unet.up_blocks.0.resnets.2.norm1.bias', 'unet.down_blocks.2.attentions.0.transformer_blocks.0.norm1.weight', 'unet.up_blocks.2.attentions.0.transformer_blocks.0.norm3.bias', 'unet.mid_block.attentions.0.transformer_blocks.0.norm2.bias', 'unet.up_blocks.0.resnets.0.norm1.bias', 'edit_head.mapper.decoder.layers.3.multihead_attn.in_proj_bias', 'unet.up_blocks.1.attentions.0.transformer_blocks.0.ff.net.2.weight', 'unet.up_blocks.3.attentions.1.transformer_blocks.0.norm3.bias', 'vae.encoder.down_blocks.0.resnets.0.conv1.bias', 'unet.up_blocks.3.resnets.2.conv1.bias', 'edit_head.mapper.encoder.layers.0.self_attn.out_proj.weight', 'unet.up_blocks.3.attentions.1.transformer_blocks.0.ff.net.2.bias', 'unet.down_blocks.2.attentions.1.transformer_blocks.0.norm3.bias', 'vae.decoder.up_blocks.0.resnets.2.conv2.weight', 'unet.up_blocks.1.attentions.2.transformer_blocks.0.ff.net.2.bias', 'unet.up_blocks.3.attentions.0.transformer_blocks.0.attn2.to_q.weight', 'edit_head.mapper.encoder.layers.2.self_attn.out_proj.weight', 'vae.decoder.mid_block.attentions.0.to_out.0.weight', 'vae.encoder.down_blocks.1.downsamplers.0.conv.weight', 'unet.down_blocks.0.attentions.1.transformer_blocks.0.ff.net.2.weight', 'unet.mid_block.resnets.1.time_emb_proj.weight', 'edit_head.mapper.decoder.layers.2.norm3.bias', 'unet.up_blocks.3.attentions.1.transformer_blocks.0.norm3.weight', 'edit_head.mapper.decoder.layers.0.norm3.bias', 'unet.up_blocks.3.attentions.2.transformer_blocks.0.norm3.weight', 'unet.up_blocks.3.attentions.0.transformer_blocks.0.attn1.to_v.weight', 'unet.up_blocks.1.resnets.1.conv1.bias', 'unet.up_blocks.3.resnets.2.conv1.weight', 'vae.decoder.up_blocks.3.resnets.0.conv1.bias', 'unet.up_blocks.1.attentions.1.transformer_blocks.0.attn1.to_q.weight', 'unet.up_blocks.1.attentions.2.proj_in.bias', 'unet.up_blocks.3.resnets.2.conv_shortcut.bias', 'unet.up_blocks.2.attentions.2.transformer_blocks.0.norm1.bias', 'unet.mid_block.attentions.0.transformer_blocks.0.ff.net.0.proj.bias', 'edit_head.mapper.decoder.layers.2.linear1.bias', 'unet.mid_block.attentions.0.transformer_blocks.0.attn2.to_q.weight', 'vae.encoder.conv_in.bias', 'edit_head.mapper.decoder.layers.1.norm1.bias', 'unet.down_blocks.1.resnets.0.conv2.bias', 'unet.mid_block.resnets.1.norm2.weight', 'unet.up_blocks.2.attentions.1.transformer_blocks.0.norm1.bias', 'vae.decoder.up_blocks.0.resnets.0.norm2.bias', 'unet.down_blocks.2.resnets.1.norm1.bias', 'unet.up_blocks.2.resnets.2.norm1.bias', 'unet.up_blocks.3.resnets.0.conv2.weight', 'unet.down_blocks.1.resnets.1.norm2.weight', 'vae.encoder.down_blocks.1.resnets.0.conv_shortcut.weight', 'unet.down_blocks.2.attentions.0.transformer_blocks.0.ff.net.2.weight', 'unet.up_blocks.2.resnets.0.conv1.weight', 'unet.down_blocks.1.attentions.1.norm.bias', 'unet.up_blocks.2.resnets.2.conv_shortcut.bias', 'vae.decoder.up_blocks.2.resnets.1.norm2.bias', 'unet.up_blocks.2.attentions.0.norm.bias', 'vae.encoder.mid_block.resnets.0.conv1.bias', 'unet.up_blocks.1.attentions.1.transformer_blocks.0.ff.net.2.weight', 'edit_head.mapper.encoder.layers.3.norm2.bias', 'vae.decoder.up_blocks.1.resnets.2.norm1.weight', 'unet.down_blocks.2.attentions.1.transformer_blocks.0.ff.net.0.proj.bias', 'unet.down_blocks.3.resnets.0.conv1.bias', 'vae.decoder.up_blocks.3.resnets.0.norm1.weight', 'vae.decoder.up_blocks.0.resnets.2.conv1.bias', 'vae.post_quant_conv.bias', 'vae.encoder.mid_block.resnets.1.norm1.weight', 'unet.up_blocks.0.resnets.2.conv2.weight', 'vae.decoder.up_blocks.0.resnets.2.conv1.weight', 'vae.encoder.down_blocks.2.resnets.0.conv_shortcut.bias', 'vae.encoder.mid_block.resnets.1.conv2.weight', 'unet.down_blocks.2.attentions.1.transformer_blocks.0.ff.net.2.weight', 'unet.down_blocks.1.attentions.1.transformer_blocks.0.norm2.weight', 'unet.down_blocks.2.attentions.0.transformer_blocks.0.ff.net.0.proj.bias', 'unet.down_blocks.0.attentions.1.transformer_blocks.0.norm3.bias', 'unet.down_blocks.1.attentions.1.norm.weight', 'unet.up_blocks.2.attentions.1.transformer_blocks.0.attn1.to_out.0.bias', 'unet.up_blocks.2.attentions.1.transformer_blocks.0.ff.net.0.proj.weight', 'edit_head.mapper.decoder.layers.2.multihead_attn.in_proj_weight', 'vae.encoder.down_blocks.3.resnets.1.conv2.weight', 'unet.down_blocks.1.attentions.0.proj_out.bias', 'unet.mid_block.resnets.1.norm1.bias', 'unet.down_blocks.1.attentions.1.transformer_blocks.0.norm1.weight', 'unet.up_blocks.2.attentions.1.transformer_blocks.0.attn2.to_q.weight', 'unet.up_blocks.2.attentions.1.transformer_blocks.0.attn2.to_v.weight', 'unet.up_blocks.1.resnets.1.conv_shortcut.bias', 'unet.down_blocks.1.attentions.0.transformer_blocks.0.norm2.weight', 'unet.down_blocks.0.attentions.0.transformer_blocks.0.ff.net.0.proj.bias', 'unet.down_blocks.2.attentions.1.transformer_blocks.0.norm2.bias', 'edit_head.mapper.decoder.layers.0.self_attn.out_proj.bias', 'vae.encoder.mid_block.attentions.0.group_norm.weight', 'unet.up_blocks.1.resnets.0.conv_shortcut.bias', 'unet.up_blocks.0.resnets.2.conv2.bias', 'unet.up_blocks.3.attentions.2.transformer_blocks.0.norm2.bias', 'vae.encoder.mid_block.attentions.0.group_norm.bias', 'unet.down_blocks.3.resnets.1.norm2.weight', 'unet.up_blocks.2.attentions.1.transformer_blocks.0.attn1.to_q.weight', 'unet.down_blocks.0.attentions.1.transformer_blocks.0.ff.net.0.proj.weight', 'unet.up_blocks.2.attentions.0.transformer_blocks.0.ff.net.0.proj.bias', 'unet.up_blocks.2.attentions.0.proj_out.weight', 'unet.down_blocks.2.attentions.1.transformer_blocks.0.ff.net.0.proj.weight', 'unet.up_blocks.2.attentions.1.transformer_blocks.0.ff.net.2.bias', 'unet.mid_block.attentions.0.transformer_blocks.0.norm1.bias', 'edit_head.mapper.decoder.layers.3.norm3.bias', 'unet.down_blocks.2.attentions.0.transformer_blocks.0.norm3.weight', 'unet.up_blocks.2.attentions.0.transformer_blocks.0.ff.net.2.weight', 'unet.down_blocks.0.resnets.1.norm2.bias', 'edit_head.mapper.encoder.layers.3.self_attn.in_proj_weight', 'unet.up_blocks.2.attentions.2.transformer_blocks.0.attn1.to_out.0.bias', 'unet.down_blocks.0.resnets.1.time_emb_proj.bias', 'unet.up_blocks.2.attentions.2.transformer_blocks.0.norm2.bias', 'unet.down_blocks.0.resnets.1.time_emb_proj.weight', 'unet.down_blocks.3.resnets.1.conv1.weight', 'unet.down_blocks.1.attentions.1.transformer_blocks.0.attn1.to_out.0.weight', 'vae.decoder.up_blocks.2.resnets.2.norm1.bias', 'unet.down_blocks.2.attentions.0.transformer_blocks.0.ff.net.2.bias', 'unet.up_blocks.1.attentions.0.transformer_blocks.0.attn2.to_q.weight', 'unet.up_blocks.3.resnets.2.norm2.weight', 'unet.up_blocks.2.attentions.0.transformer_blocks.0.attn1.to_v.weight', 'unet.up_blocks.1.resnets.0.conv_shortcut.weight', 'unet.up_blocks.1.attentions.2.transformer_blocks.0.norm2.weight', 'unet.up_blocks.0.resnets.1.conv2.bias', 'vae.decoder.up_blocks.2.resnets.1.norm1.bias', 'vae.decoder.up_blocks.2.resnets.2.conv1.weight', 'unet.up_blocks.2.attentions.1.transformer_blocks.0.norm2.weight', 'unet.down_blocks.0.attentions.1.norm.bias', 'unet.up_blocks.0.resnets.1.norm1.weight', 'vae.decoder.up_blocks.2.upsamplers.0.conv.weight', 'unet.up_blocks.1.attentions.0.transformer_blocks.0.attn2.to_out.0.bias', 'vae.encoder.down_blocks.1.resnets.0.norm2.weight', 'vae.decoder.up_blocks.2.resnets.2.conv2.weight', 'vae.decoder.up_blocks.2.upsamplers.0.conv.bias', 'edit_head.mapper.encoder.layers.2.self_attn.in_proj_bias', 'unet.down_blocks.1.resnets.1.time_emb_proj.weight', 'unet.down_blocks.0.attentions.0.transformer_blocks.0.norm3.bias', 'edit_head.mapper.encoder.layers.1.self_attn.out_proj.weight', 'vae.decoder.up_blocks.3.resnets.1.norm1.bias', 'unet.up_blocks.2.attentions.1.transformer_blocks.0.ff.net.2.weight', 'unet.down_blocks.2.attentions.0.transformer_blocks.0.attn2.to_k.weight', 'unet.up_blocks.0.resnets.0.conv2.bias', 'edit_head.mapper.decoder.layers.3.multihead_attn.out_proj.bias', 'unet.down_blocks.1.resnets.0.time_emb_proj.bias', 'unet.down_blocks.2.attentions.0.transformer_blocks.0.norm2.bias', 'unet.mid_block.attentions.0.norm.bias', 'unet.mid_block.attentions.0.transformer_blocks.0.attn1.to_out.0.weight', 'unet.down_blocks.0.attentions.0.transformer_blocks.0.norm2.bias', 'unet.up_blocks.1.resnets.0.norm1.bias', 'edit_head.mapper.encoder.layers.2.norm1.bias', 'unet.mid_block.attentions.0.proj_out.bias', 'unet.down_blocks.0.attentions.0.transformer_blocks.0.attn1.to_out.0.weight', 'unet.up_blocks.1.attentions.2.transformer_blocks.0.norm3.bias', 'unet.down_blocks.1.resnets.0.norm2.bias', 'unet.up_blocks.1.attentions.0.proj_in.bias', 'unet.down_blocks.1.attentions.0.transformer_blocks.0.attn2.to_k.weight', 'unet.up_blocks.1.resnets.1.conv_shortcut.weight', 'unet.up_blocks.3.attentions.0.transformer_blocks.0.norm3.bias', 'unet.up_blocks.3.attentions.1.transformer_blocks.0.attn1.to_out.0.bias', 'vae.encoder.down_blocks.2.resnets.1.norm1.weight', 'unet.up_blocks.2.resnets.1.norm2.bias', 'edit_head.mapper.decoder.layers.2.multihead_attn.in_proj_bias', 'edit_head.mapper.encoder.layers.3.linear2.bias', 'vae.encoder.down_blocks.2.resnets.0.norm1.bias', 'vae.decoder.up_blocks.1.resnets.2.norm1.bias', 'vae.encoder.conv_out.weight', 'unet.up_blocks.3.attentions.2.transformer_blocks.0.ff.net.0.proj.weight', 'unet.up_blocks.1.resnets.2.norm1.weight', 'vae.encoder.down_blocks.1.resnets.0.conv1.bias', 'unet.up_blocks.3.attentions.1.transformer_blocks.0.norm2.weight', 'vae.decoder.up_blocks.0.resnets.0.norm1.weight', 'unet.up_blocks.1.attentions.2.norm.bias', 'unet.up_blocks.3.attentions.1.norm.weight', 'unet.up_blocks.3.attentions.1.transformer_blocks.0.attn2.to_k.weight', 'unet.down_blocks.2.resnets.1.norm2.bias', 'unet.up_blocks.0.resnets.0.norm2.bias', 'edit_head.mapper.encoder.layers.3.self_attn.out_proj.weight', 'unet.down_blocks.1.attentions.1.transformer_blocks.0.attn1.to_v.weight', 'vae.decoder.up_blocks.3.resnets.2.norm1.bias', 'unet.down_blocks.2.attentions.1.transformer_blocks.0.norm1.bias', 'unet.up_blocks.1.attentions.2.transformer_blocks.0.norm2.bias', 'unet.mid_block.resnets.0.conv2.weight', 'unet.up_blocks.3.attentions.0.proj_in.bias', 'unet.down_blocks.1.attentions.1.transformer_blocks.0.attn1.to_q.weight', 'unet.up_blocks.0.resnets.2.conv1.bias', 'unet.down_blocks.0.attentions.0.proj_in.weight', 'edit_head.mapper.encoder.layers.2.norm1.weight', 'unet.up_blocks.2.resnets.1.conv1.bias', 'unet.up_blocks.3.attentions.2.proj_in.weight', 'vae.decoder.up_blocks.0.resnets.2.norm2.weight', 'unet.up_blocks.1.resnets.1.norm1.bias', 'unet.up_blocks.2.resnets.0.time_emb_proj.bias', 'unet.mid_block.resnets.1.conv2.bias', 'unet.up_blocks.2.attentions.2.transformer_blocks.0.attn1.to_k.weight', 'unet.down_blocks.0.attentions.0.proj_out.weight', 'unet.up_blocks.3.attentions.2.transformer_blocks.0.norm3.bias', 'vae.encoder.mid_block.resnets.0.conv1.weight', 'unet.down_blocks.2.resnets.0.norm2.bias', 'vae.encoder.down_blocks.3.resnets.0.conv2.weight', 'unet.down_blocks.2.attentions.0.norm.bias', 'vae.decoder.mid_block.resnets.1.norm1.weight', 'unet.down_blocks.2.resnets.0.norm1.weight', 'unet.mid_block.attentions.0.proj_in.weight', 'unet.down_blocks.0.attentions.0.norm.weight', 'unet.mid_block.attentions.0.transformer_blocks.0.attn2.to_out.0.bias', 'unet.up_blocks.3.attentions.2.transformer_blocks.0.attn2.to_k.weight', 'vae.encoder.mid_block.resnets.1.norm2.bias', 'edit_head.mapper.decoder.layers.2.self_attn.in_proj_bias', 'unet.down_blocks.1.attentions.0.transformer_blocks.0.norm3.bias', 'unet.up_blocks.1.attentions.1.proj_in.bias', 'unet.down_blocks.0.attentions.1.transformer_blocks.0.norm1.weight', 'edit_head.mapper.decoder.layers.3.linear1.bias', 'unet.up_blocks.3.attentions.2.transformer_blocks.0.norm1.weight', 'unet.down_blocks.2.attentions.1.norm.weight', 'unet.down_blocks.1.resnets.1.conv1.bias', 'vae.decoder.up_blocks.3.resnets.0.norm1.bias', 'vae.decoder.conv_norm_out.weight', 'edit_head.mapper.encoder.layers.0.linear1.weight', 'unet.up_blocks.3.resnets.2.time_emb_proj.bias', 'vae.decoder.up_blocks.1.upsamplers.0.conv.weight', 'vae.encoder.down_blocks.3.resnets.0.norm1.bias', 'unet.up_blocks.3.attentions.0.transformer_blocks.0.attn2.to_k.weight', 'edit_head.mapper.decoder.norm.weight', 'unet.down_blocks.1.attentions.1.transformer_blocks.0.norm2.bias', 'vae.decoder.up_blocks.1.resnets.0.conv2.bias', 'vae.encoder.down_blocks.1.resnets.1.conv2.bias', 'unet.up_blocks.0.resnets.1.conv1.weight', 'unet.up_blocks.1.attentions.0.transformer_blocks.0.attn2.to_k.weight', 'unet.down_blocks.0.resnets.0.norm1.bias', 'unet.up_blocks.1.attentions.1.transformer_blocks.0.ff.net.2.bias', 'edit_head.mapper.decoder.layers.2.self_attn.out_proj.weight', 'unet.up_blocks.3.resnets.0.conv1.weight', 'unet.up_blocks.1.attentions.2.transformer_blocks.0.ff.net.0.proj.weight', 'unet.up_blocks.2.attentions.2.transformer_blocks.0.ff.net.0.proj.weight', 'unet.down_blocks.1.resnets.0.conv1.weight', 'unet.time_embedding.linear_1.bias', 'vae.decoder.up_blocks.2.resnets.0.conv2.weight', 'unet.down_blocks.1.attentions.0.transformer_blocks.0.attn1.to_v.weight', 'unet.down_blocks.2.attentions.0.proj_in.bias', 'vae.encoder.down_blocks.2.resnets.0.conv2.weight', 'vae.decoder.up_blocks.1.resnets.1.conv1.bias', 'unet.up_blocks.2.resnets.0.norm2.weight', 'vae.encoder.down_blocks.3.resnets.1.conv1.weight', 'unet.down_blocks.0.attentions.0.transformer_blocks.0.norm1.bias', 'vae.decoder.up_blocks.1.resnets.0.conv1.weight', 'unet.up_blocks.1.attentions.1.proj_out.bias', 'vae.encoder.down_blocks.1.resnets.0.norm1.bias', 'unet.up_blocks.2.resnets.0.conv_shortcut.weight', 'vae.encoder.down_blocks.3.resnets.0.conv1.bias', 'edit_head.mapper.encoder.layers.0.linear1.bias', 'edit_head.mapper.encoder.layers.3.linear1.bias', 'unet.down_blocks.2.attentions.1.transformer_blocks.0.attn2.to_out.0.weight', 'unet.down_blocks.0.resnets.1.conv2.weight', 'unet.up_blocks.3.attentions.1.transformer_blocks.0.attn2.to_out.0.bias', 'vae.decoder.up_blocks.2.resnets.0.conv2.bias', 'vae.encoder.mid_block.resnets.0.conv2.bias', 'edit_head.mapper.decoder.layers.1.self_attn.in_proj_bias', 'unet.up_blocks.2.resnets.2.time_emb_proj.bias', 'unet.down_blocks.1.attentions.1.transformer_blocks.0.attn1.to_out.0.bias', 'unet.up_blocks.0.resnets.1.conv2.weight', 'unet.down_blocks.2.attentions.0.norm.weight', 'unet.up_blocks.2.attentions.2.transformer_blocks.0.ff.net.2.weight', 'unet.up_blocks.2.attentions.1.transformer_blocks.0.attn1.to_out.0.weight', 'edit_head.mapper.decoder.layers.2.self_attn.out_proj.bias', 'vae.encoder.down_blocks.2.resnets.0.norm2.bias', 'edit_head.mapper.decoder.layers.2.norm2.bias', 'unet.up_blocks.1.attentions.1.transformer_blocks.0.attn2.to_q.weight', 'unet.down_blocks.3.resnets.0.norm2.bias', 'unet.mid_block.attentions.0.transformer_blocks.0.norm2.weight', 'unet.up_blocks.2.attentions.1.proj_out.bias', 'unet.down_blocks.2.resnets.1.conv1.bias', 'vae.encoder.down_blocks.0.resnets.1.conv2.weight', 'unet.down_blocks.3.resnets.1.norm1.weight', 'unet.up_blocks.2.attentions.2.transformer_blocks.0.ff.net.2.bias', 'vae.encoder.down_blocks.0.resnets.0.norm2.bias', 'edit_head.mapper.decoder.layers.0.linear1.weight', 'unet.up_blocks.1.attentions.1.transformer_blocks.0.norm2.bias', 'unet.down_blocks.2.attentions.1.proj_in.bias', 'unet.up_blocks.2.attentions.0.transformer_blocks.0.attn1.to_q.weight', 'unet.up_blocks.2.attentions.1.norm.weight', 'vae.decoder.up_blocks.2.resnets.0.norm1.weight', 'vae.decoder.mid_block.resnets.1.conv1.bias', 'unet.down_blocks.0.attentions.1.transformer_blocks.0.attn1.to_q.weight', 'unet.down_blocks.0.attentions.1.norm.weight', 'unet.up_blocks.2.attentions.0.proj_in.bias', 'unet.up_blocks.2.attentions.2.norm.bias', 'edit_head.mapper.decoder.layers.3.multihead_attn.in_proj_weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of the model checkpoint at ../autodl-tmp/openai/clip-vit-large-patch14 were not used when initializing CLIPVisionModel: ['text_model.encoder.layers.11.self_attn.k_proj.weight', 'text_model.encoder.layers.1.self_attn.v_proj.bias', 'text_model.encoder.layers.4.self_attn.v_proj.weight', 'text_model.encoder.layers.3.layer_norm2.weight', 'text_model.encoder.layers.1.self_attn.k_proj.bias', 'text_model.encoder.layers.0.layer_norm2.bias', 'text_model.encoder.layers.4.mlp.fc2.weight', 'text_model.encoder.layers.3.layer_norm1.weight', 'text_model.encoder.layers.10.self_attn.v_proj.weight', 'text_model.encoder.layers.5.self_attn.v_proj.weight', 'text_model.encoder.layers.7.layer_norm2.bias', 'text_model.encoder.layers.8.mlp.fc2.bias', 'text_model.encoder.layers.7.self_attn.k_proj.weight', 'visual_projection.weight', 'text_model.encoder.layers.0.self_attn.v_proj.weight', 'text_model.encoder.layers.1.self_attn.k_proj.weight', 'text_model.encoder.layers.2.mlp.fc1.weight', 'text_model.encoder.layers.9.mlp.fc2.bias', 'text_model.encoder.layers.6.self_attn.q_proj.weight', 'text_model.encoder.layers.3.self_attn.k_proj.bias', 'text_model.encoder.layers.9.self_attn.q_proj.bias', 'text_model.encoder.layers.11.self_attn.q_proj.bias', 'text_model.encoder.layers.0.self_attn.v_proj.bias', 'text_model.encoder.layers.0.layer_norm2.weight', 'text_model.encoder.layers.3.mlp.fc2.weight', 'text_model.encoder.layers.11.mlp.fc2.bias', 'text_model.encoder.layers.9.mlp.fc2.weight', 'text_model.encoder.layers.7.self_attn.q_proj.bias', 'text_model.encoder.layers.3.self_attn.v_proj.weight', 'text_model.encoder.layers.6.layer_norm2.bias', 'text_model.encoder.layers.7.mlp.fc1.weight', 'text_model.encoder.layers.4.self_attn.v_proj.bias', 'text_model.encoder.layers.11.layer_norm1.bias', 'text_model.encoder.layers.0.self_attn.out_proj.bias', 'text_model.encoder.layers.9.self_attn.out_proj.bias', 'text_model.encoder.layers.10.layer_norm2.weight', 'text_model.encoder.layers.5.layer_norm2.bias', 'text_model.encoder.layers.6.layer_norm2.weight', 'text_model.encoder.layers.11.self_attn.k_proj.bias', 'text_model.encoder.layers.6.mlp.fc2.weight', 'text_model.encoder.layers.4.layer_norm1.weight', 'text_model.encoder.layers.0.mlp.fc1.weight', 'text_model.encoder.layers.10.self_attn.q_proj.weight', 'text_model.encoder.layers.10.layer_norm2.bias', 'text_model.encoder.layers.9.layer_norm2.bias', 'text_model.encoder.layers.3.self_attn.v_proj.bias', 'text_model.encoder.layers.5.self_attn.k_proj.bias', 'text_model.encoder.layers.8.self_attn.v_proj.weight', 'text_model.encoder.layers.2.layer_norm2.weight', 'text_model.encoder.layers.6.self_attn.v_proj.weight', 'text_model.embeddings.position_embedding.weight', 'text_model.encoder.layers.8.layer_norm1.weight', 'text_model.encoder.layers.8.mlp.fc2.weight', 'text_model.encoder.layers.1.self_attn.out_proj.weight', 'text_model.encoder.layers.3.self_attn.out_proj.bias', 'text_model.encoder.layers.11.self_attn.out_proj.weight', 'text_projection.weight', 'text_model.encoder.layers.2.mlp.fc2.bias', 'text_model.encoder.layers.2.self_attn.k_proj.bias', 'text_model.encoder.layers.10.mlp.fc2.bias', 'text_model.encoder.layers.2.self_attn.v_proj.bias', 'text_model.encoder.layers.3.mlp.fc2.bias', 'text_model.encoder.layers.5.self_attn.out_proj.weight', 'text_model.encoder.layers.10.layer_norm1.weight', 'text_model.encoder.layers.5.self_attn.q_proj.weight', 'text_model.encoder.layers.5.layer_norm2.weight', 'text_model.encoder.layers.5.layer_norm1.weight', 'text_model.encoder.layers.10.self_attn.out_proj.weight', 'text_model.encoder.layers.11.layer_norm2.weight', 'text_model.encoder.layers.5.self_attn.v_proj.bias', 'text_model.encoder.layers.0.mlp.fc2.weight', 'text_model.encoder.layers.9.self_attn.k_proj.weight', 'text_model.encoder.layers.4.self_attn.out_proj.weight', 'text_model.encoder.layers.0.layer_norm1.weight', 'text_model.encoder.layers.10.self_attn.k_proj.weight', 'text_model.encoder.layers.5.mlp.fc2.weight', 'text_model.encoder.layers.6.mlp.fc1.weight', 'text_model.encoder.layers.2.self_attn.q_proj.weight', 'text_model.encoder.layers.7.layer_norm1.bias', 'text_model.encoder.layers.2.self_attn.out_proj.bias', 'text_model.encoder.layers.1.self_attn.q_proj.bias', 'text_model.encoder.layers.10.mlp.fc1.weight', 'text_model.encoder.layers.3.layer_norm2.bias', 'text_model.encoder.layers.10.mlp.fc2.weight', 'text_model.encoder.layers.8.self_attn.v_proj.bias', 'text_model.encoder.layers.3.mlp.fc1.weight', 'text_model.encoder.layers.3.self_attn.out_proj.weight', 'text_model.encoder.layers.3.self_attn.k_proj.weight', 'text_model.encoder.layers.8.self_attn.q_proj.bias', 'text_model.encoder.layers.8.layer_norm2.bias', 'text_model.encoder.layers.4.mlp.fc1.weight', 'text_model.encoder.layers.8.self_attn.q_proj.weight', 'text_model.encoder.layers.0.mlp.fc2.bias', 'text_model.encoder.layers.10.self_attn.k_proj.bias', 'text_model.encoder.layers.4.layer_norm2.weight', 'text_model.encoder.layers.4.layer_norm2.bias', 'text_model.encoder.layers.10.self_attn.v_proj.bias', 'text_model.encoder.layers.0.self_attn.q_proj.bias', 'text_model.encoder.layers.11.layer_norm2.bias', 'text_model.encoder.layers.4.mlp.fc2.bias', 'text_model.encoder.layers.8.layer_norm1.bias', 'text_model.encoder.layers.7.layer_norm1.weight', 'text_model.encoder.layers.11.self_attn.v_proj.bias', 'text_model.encoder.layers.1.mlp.fc1.weight', 'text_model.encoder.layers.9.self_attn.v_proj.bias', 'text_model.encoder.layers.11.mlp.fc1.bias', 'text_model.encoder.layers.0.self_attn.q_proj.weight', 'text_model.encoder.layers.2.self_attn.out_proj.weight', 'text_model.encoder.layers.1.layer_norm1.bias', 'text_model.encoder.layers.11.self_attn.v_proj.weight', 'text_model.encoder.layers.7.self_attn.q_proj.weight', 'text_model.final_layer_norm.bias', 'text_model.encoder.layers.1.self_attn.q_proj.weight', 'text_model.encoder.layers.7.self_attn.v_proj.weight', 'logit_scale', 'text_model.encoder.layers.7.mlp.fc2.bias', 'text_model.encoder.layers.5.mlp.fc2.bias', 'text_model.encoder.layers.0.self_attn.k_proj.bias', 'text_model.encoder.layers.2.mlp.fc1.bias', 'text_model.encoder.layers.11.mlp.fc2.weight', 'text_model.encoder.layers.10.mlp.fc1.bias', 'text_model.encoder.layers.1.layer_norm2.bias', 'text_model.encoder.layers.3.mlp.fc1.bias', 'text_model.encoder.layers.7.self_attn.out_proj.bias', 'text_model.encoder.layers.7.mlp.fc1.bias', 'text_model.encoder.layers.0.self_attn.k_proj.weight', 'text_model.encoder.layers.5.mlp.fc1.bias', 'text_model.encoder.layers.6.self_attn.q_proj.bias', 'text_model.encoder.layers.6.self_attn.k_proj.bias', 'text_model.encoder.layers.8.layer_norm2.weight', 'text_model.encoder.layers.4.self_attn.out_proj.bias', 'text_model.encoder.layers.4.mlp.fc1.bias', 'text_model.encoder.layers.9.mlp.fc1.weight', 'text_model.encoder.layers.1.mlp.fc1.bias', 'text_model.encoder.layers.2.mlp.fc2.weight', 'text_model.encoder.layers.6.self_attn.out_proj.bias', 'text_model.encoder.layers.5.self_attn.q_proj.bias', 'text_model.encoder.layers.6.mlp.fc2.bias', 'text_model.encoder.layers.0.mlp.fc1.bias', 'text_model.encoder.layers.5.self_attn.k_proj.weight', 'text_model.encoder.layers.2.self_attn.v_proj.weight', 'text_model.encoder.layers.0.layer_norm1.bias', 'text_model.encoder.layers.5.mlp.fc1.weight', 'text_model.encoder.layers.6.self_attn.k_proj.weight', 'text_model.encoder.layers.9.layer_norm1.weight', 'text_model.encoder.layers.1.self_attn.v_proj.weight', 'text_model.encoder.layers.10.self_attn.q_proj.bias', 'text_model.encoder.layers.6.self_attn.out_proj.weight', 'text_model.encoder.layers.1.mlp.fc2.weight', 'text_model.encoder.layers.0.self_attn.out_proj.weight', 'text_model.encoder.layers.9.layer_norm1.bias', 'text_model.encoder.layers.6.mlp.fc1.bias', 'text_model.encoder.layers.4.self_attn.q_proj.bias', 'text_model.embeddings.position_ids', 'text_model.encoder.layers.1.layer_norm2.weight', 'text_model.encoder.layers.4.self_attn.q_proj.weight', 'text_model.encoder.layers.3.self_attn.q_proj.bias', 'text_model.encoder.layers.8.self_attn.out_proj.weight', 'text_model.encoder.layers.8.mlp.fc1.bias', 'text_model.encoder.layers.5.self_attn.out_proj.bias', 'text_model.encoder.layers.6.layer_norm1.weight', 'text_model.encoder.layers.2.layer_norm1.bias', 'text_model.encoder.layers.10.self_attn.out_proj.bias', 'text_model.encoder.layers.11.self_attn.q_proj.weight', 'text_model.encoder.layers.11.self_attn.out_proj.bias', 'text_model.encoder.layers.1.layer_norm1.weight', 'text_model.encoder.layers.1.self_attn.out_proj.bias', 'text_model.encoder.layers.7.self_attn.out_proj.weight', 'text_model.encoder.layers.10.layer_norm1.bias', 'text_model.embeddings.token_embedding.weight', 'text_model.encoder.layers.5.layer_norm1.bias', 'text_model.encoder.layers.4.layer_norm1.bias', 'text_model.encoder.layers.7.layer_norm2.weight', 'text_model.encoder.layers.9.mlp.fc1.bias', 'text_model.encoder.layers.1.mlp.fc2.bias', 'text_model.encoder.layers.2.self_attn.q_proj.bias', 'text_model.encoder.layers.9.layer_norm2.weight', 'text_model.encoder.layers.4.self_attn.k_proj.weight', 'text_model.final_layer_norm.weight', 'text_model.encoder.layers.2.layer_norm1.weight', 'text_model.encoder.layers.9.self_attn.out_proj.weight', 'text_model.encoder.layers.9.self_attn.v_proj.weight', 'text_model.encoder.layers.6.layer_norm1.bias', 'text_model.encoder.layers.2.layer_norm2.bias', 'text_model.encoder.layers.9.self_attn.q_proj.weight', 'text_model.encoder.layers.6.self_attn.v_proj.bias', 'text_model.encoder.layers.11.layer_norm1.weight', 'text_model.encoder.layers.3.layer_norm1.bias', 'text_model.encoder.layers.9.self_attn.k_proj.bias', 'text_model.encoder.layers.8.self_attn.k_proj.weight', 'text_model.encoder.layers.8.self_attn.k_proj.bias', 'text_model.encoder.layers.4.self_attn.k_proj.bias', 'text_model.encoder.layers.7.self_attn.v_proj.bias', 'text_model.encoder.layers.3.self_attn.q_proj.weight', 'text_model.encoder.layers.8.mlp.fc1.weight', 'text_model.encoder.layers.8.self_attn.out_proj.bias', 'text_model.encoder.layers.2.self_attn.k_proj.weight', 'text_model.encoder.layers.7.mlp.fc2.weight', 'text_model.encoder.layers.7.self_attn.k_proj.bias', 'text_model.encoder.layers.11.mlp.fc1.weight']\n",
      "- This IS expected if you are initializing CLIPVisionModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing CLIPVisionModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "DEFAULT_IMAGE_TOKEN = '<image>'\n",
    "DEFAULT_IMAGE_PATCH_TOKEN = '<im_patch>'\n",
    "DEFAULT_IM_START_TOKEN = '<im_start>'\n",
    "DEFAULT_IM_END_TOKEN = '<im_end>'\n",
    "\n",
    "MODEL_NAME = '../autodl-tmp/LLaVA-7B-v1'\n",
    "model_name = os.path.expanduser(MODEL_NAME)\n",
    "\n",
    "tokenizer = transformers.AutoTokenizer.from_pretrained(model_name)\n",
    "model = LlavaLlamaForCausalLM.from_pretrained(model_name, low_cpu_mem_usage=True, torch_dtype=T.float16, use_cache=True).cuda()\n",
    "image_processor = transformers.CLIPImageProcessor.from_pretrained(model.config.mm_vision_tower, torch_dtype=T.float16)\n",
    "\n",
    "tokenizer.padding_side = 'left'\n",
    "\n",
    "mm_use_im_start_end = getattr(model.config, 'mm_use_im_start_end', False)\n",
    "tokenizer.add_tokens([DEFAULT_IMAGE_PATCH_TOKEN], special_tokens=True)\n",
    "if mm_use_im_start_end: tokenizer.add_tokens([DEFAULT_IM_START_TOKEN, DEFAULT_IM_END_TOKEN], special_tokens=True)\n",
    "\n",
    "vision_tower = model.get_model().vision_tower[0]\n",
    "vision_tower = transformers.CLIPVisionModel.from_pretrained(vision_tower.config._name_or_path, torch_dtype=T.float16, low_cpu_mem_usage=True).cuda()\n",
    "model.get_model().vision_tower[0] = vision_tower\n",
    "vision_config = vision_tower.config\n",
    "vision_config.im_patch_token = tokenizer.convert_tokens_to_ids([DEFAULT_IMAGE_PATCH_TOKEN])[0]\n",
    "vision_config.use_im_start_end = mm_use_im_start_end\n",
    "if mm_use_im_start_end: vision_config.im_start_token, vision_config.im_end_token = tokenizer.convert_tokens_to_ids([DEFAULT_IM_START_TOKEN, DEFAULT_IM_END_TOKEN])\n",
    "image_token_len = (vision_config.image_size//vision_config.patch_size)**2\n",
    "\n",
    "_ = model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8e479793-f2f9-45b6-a05b-9f5b0d09e396",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "OSError",
     "evalue": "We couldn't connect to 'https://huggingface.co' to load this file, couldn't find it in the cached files and it looks like jordiclive/flan-t5-11b-summarizer-filtered is not the path to a directory containing a file named config.json.\nCheckout your internet connection or see how to run the library in offline mode at 'https://huggingface.co/docs/transformers/installation#offline-mode'.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTimeoutError\u001b[0m                              Traceback (most recent call last)",
      "File \u001b[0;32m~/miniconda3/envs/mgie/lib/python3.10/site-packages/urllib3/connection.py:198\u001b[0m, in \u001b[0;36mHTTPConnection._new_conn\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    197\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 198\u001b[0m     sock \u001b[38;5;241m=\u001b[39m \u001b[43mconnection\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate_connection\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    199\u001b[0m \u001b[43m        \u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dns_host\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mport\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    200\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    201\u001b[0m \u001b[43m        \u001b[49m\u001b[43msource_address\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msource_address\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    202\u001b[0m \u001b[43m        \u001b[49m\u001b[43msocket_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msocket_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    203\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    204\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m socket\u001b[38;5;241m.\u001b[39mgaierror \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m~/miniconda3/envs/mgie/lib/python3.10/site-packages/urllib3/util/connection.py:85\u001b[0m, in \u001b[0;36mcreate_connection\u001b[0;34m(address, timeout, source_address, socket_options)\u001b[0m\n\u001b[1;32m     84\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 85\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m err\n\u001b[1;32m     86\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     87\u001b[0m     \u001b[38;5;66;03m# Break explicitly a reference cycle\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/mgie/lib/python3.10/site-packages/urllib3/util/connection.py:73\u001b[0m, in \u001b[0;36mcreate_connection\u001b[0;34m(address, timeout, source_address, socket_options)\u001b[0m\n\u001b[1;32m     72\u001b[0m     sock\u001b[38;5;241m.\u001b[39mbind(source_address)\n\u001b[0;32m---> 73\u001b[0m \u001b[43msock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconnect\u001b[49m\u001b[43m(\u001b[49m\u001b[43msa\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     74\u001b[0m \u001b[38;5;66;03m# Break explicitly a reference cycle\u001b[39;00m\n",
      "\u001b[0;31mTimeoutError\u001b[0m: timed out",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mConnectTimeoutError\u001b[0m                       Traceback (most recent call last)",
      "File \u001b[0;32m~/miniconda3/envs/mgie/lib/python3.10/site-packages/urllib3/connectionpool.py:793\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[0;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, preload_content, decode_content, **response_kw)\u001b[0m\n\u001b[1;32m    792\u001b[0m \u001b[38;5;66;03m# Make the request on the HTTPConnection object\u001b[39;00m\n\u001b[0;32m--> 793\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    794\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    795\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    796\u001b[0m \u001b[43m    \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    797\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout_obj\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    798\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    799\u001b[0m \u001b[43m    \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    800\u001b[0m \u001b[43m    \u001b[49m\u001b[43mchunked\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchunked\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    801\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mretries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    802\u001b[0m \u001b[43m    \u001b[49m\u001b[43mresponse_conn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresponse_conn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    803\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpreload_content\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    804\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecode_content\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    805\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mresponse_kw\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    806\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    808\u001b[0m \u001b[38;5;66;03m# Everything went great!\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/mgie/lib/python3.10/site-packages/urllib3/connectionpool.py:491\u001b[0m, in \u001b[0;36mHTTPConnectionPool._make_request\u001b[0;34m(self, conn, method, url, body, headers, retries, timeout, chunked, response_conn, preload_content, decode_content, enforce_content_length)\u001b[0m\n\u001b[1;32m    490\u001b[0m         new_e \u001b[38;5;241m=\u001b[39m _wrap_proxy_error(new_e, conn\u001b[38;5;241m.\u001b[39mproxy\u001b[38;5;241m.\u001b[39mscheme)\n\u001b[0;32m--> 491\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m new_e\n\u001b[1;32m    493\u001b[0m \u001b[38;5;66;03m# conn.request() calls http.client.*.request, not the method in\u001b[39;00m\n\u001b[1;32m    494\u001b[0m \u001b[38;5;66;03m# urllib3.request. It also calls makefile (recv) on the socket.\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/mgie/lib/python3.10/site-packages/urllib3/connectionpool.py:467\u001b[0m, in \u001b[0;36mHTTPConnectionPool._make_request\u001b[0;34m(self, conn, method, url, body, headers, retries, timeout, chunked, response_conn, preload_content, decode_content, enforce_content_length)\u001b[0m\n\u001b[1;32m    466\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 467\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_validate_conn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    468\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (SocketTimeout, BaseSSLError) \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m~/miniconda3/envs/mgie/lib/python3.10/site-packages/urllib3/connectionpool.py:1099\u001b[0m, in \u001b[0;36mHTTPSConnectionPool._validate_conn\u001b[0;34m(self, conn)\u001b[0m\n\u001b[1;32m   1098\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m conn\u001b[38;5;241m.\u001b[39mis_closed:\n\u001b[0;32m-> 1099\u001b[0m     \u001b[43mconn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconnect\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1101\u001b[0m \u001b[38;5;66;03m# TODO revise this, see https://github.com/urllib3/urllib3/issues/2791\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/mgie/lib/python3.10/site-packages/urllib3/connection.py:616\u001b[0m, in \u001b[0;36mHTTPSConnection.connect\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    615\u001b[0m sock: socket\u001b[38;5;241m.\u001b[39msocket \u001b[38;5;241m|\u001b[39m ssl\u001b[38;5;241m.\u001b[39mSSLSocket\n\u001b[0;32m--> 616\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msock \u001b[38;5;241m=\u001b[39m sock \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_new_conn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    617\u001b[0m server_hostname: \u001b[38;5;28mstr\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhost\n",
      "File \u001b[0;32m~/miniconda3/envs/mgie/lib/python3.10/site-packages/urllib3/connection.py:207\u001b[0m, in \u001b[0;36mHTTPConnection._new_conn\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    206\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m SocketTimeout \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m--> 207\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m ConnectTimeoutError(\n\u001b[1;32m    208\u001b[0m         \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    209\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mConnection to \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhost\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m timed out. (connect timeout=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtimeout\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m)\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    210\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n\u001b[1;32m    212\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[0;31mConnectTimeoutError\u001b[0m: (<urllib3.connection.HTTPSConnection object at 0x7fa7a8e56200>, 'Connection to huggingface.co timed out. (connect timeout=10)')",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mMaxRetryError\u001b[0m                             Traceback (most recent call last)",
      "File \u001b[0;32m~/miniconda3/envs/mgie/lib/python3.10/site-packages/requests/adapters.py:486\u001b[0m, in \u001b[0;36mHTTPAdapter.send\u001b[0;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[1;32m    485\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 486\u001b[0m     resp \u001b[38;5;241m=\u001b[39m \u001b[43mconn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43murlopen\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    487\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    488\u001b[0m \u001b[43m        \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    489\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    490\u001b[0m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    491\u001b[0m \u001b[43m        \u001b[49m\u001b[43mredirect\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    492\u001b[0m \u001b[43m        \u001b[49m\u001b[43massert_same_host\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    493\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    494\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    495\u001b[0m \u001b[43m        \u001b[49m\u001b[43mretries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_retries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    496\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    497\u001b[0m \u001b[43m        \u001b[49m\u001b[43mchunked\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchunked\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    498\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    500\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (ProtocolError, \u001b[38;5;167;01mOSError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[0;32m~/miniconda3/envs/mgie/lib/python3.10/site-packages/urllib3/connectionpool.py:847\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[0;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, preload_content, decode_content, **response_kw)\u001b[0m\n\u001b[1;32m    845\u001b[0m     new_e \u001b[38;5;241m=\u001b[39m ProtocolError(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mConnection aborted.\u001b[39m\u001b[38;5;124m\"\u001b[39m, new_e)\n\u001b[0;32m--> 847\u001b[0m retries \u001b[38;5;241m=\u001b[39m \u001b[43mretries\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mincrement\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    848\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merror\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnew_e\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_pool\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_stacktrace\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msys\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexc_info\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[1;32m    849\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    850\u001b[0m retries\u001b[38;5;241m.\u001b[39msleep()\n",
      "File \u001b[0;32m~/miniconda3/envs/mgie/lib/python3.10/site-packages/urllib3/util/retry.py:515\u001b[0m, in \u001b[0;36mRetry.increment\u001b[0;34m(self, method, url, response, error, _pool, _stacktrace)\u001b[0m\n\u001b[1;32m    514\u001b[0m     reason \u001b[38;5;241m=\u001b[39m error \u001b[38;5;129;01mor\u001b[39;00m ResponseError(cause)\n\u001b[0;32m--> 515\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m MaxRetryError(_pool, url, reason) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mreason\u001b[39;00m  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n\u001b[1;32m    517\u001b[0m log\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIncremented Retry for (url=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m): \u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, url, new_retry)\n",
      "\u001b[0;31mMaxRetryError\u001b[0m: HTTPSConnectionPool(host='huggingface.co', port=443): Max retries exceeded with url: /jordiclive/flan-t5-11b-summarizer-filtered/resolve/main/config.json (Caused by ConnectTimeoutError(<urllib3.connection.HTTPSConnection object at 0x7fa7a8e56200>, 'Connection to huggingface.co timed out. (connect timeout=10)'))",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mConnectTimeout\u001b[0m                            Traceback (most recent call last)",
      "File \u001b[0;32m~/miniconda3/envs/mgie/lib/python3.10/site-packages/huggingface_hub/file_download.py:1261\u001b[0m, in \u001b[0;36mhf_hub_download\u001b[0;34m(repo_id, filename, subfolder, repo_type, revision, library_name, library_version, cache_dir, local_dir, local_dir_use_symlinks, user_agent, force_download, force_filename, proxies, etag_timeout, resume_download, token, local_files_only, headers, legacy_cache_layout, endpoint)\u001b[0m\n\u001b[1;32m   1260\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1261\u001b[0m     metadata \u001b[38;5;241m=\u001b[39m \u001b[43mget_hf_file_metadata\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1262\u001b[0m \u001b[43m        \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1263\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1264\u001b[0m \u001b[43m        \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1265\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43metag_timeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1266\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlibrary_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlibrary_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1267\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlibrary_version\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlibrary_version\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1268\u001b[0m \u001b[43m        \u001b[49m\u001b[43muser_agent\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muser_agent\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1269\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1270\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m EntryNotFoundError \u001b[38;5;28;01mas\u001b[39;00m http_error:\n\u001b[1;32m   1271\u001b[0m     \u001b[38;5;66;03m# Cache the non-existence of the file and raise\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/mgie/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py:119\u001b[0m, in \u001b[0;36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    117\u001b[0m     kwargs \u001b[38;5;241m=\u001b[39m smoothly_deprecate_use_auth_token(fn_name\u001b[38;5;241m=\u001b[39mfn\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, has_token\u001b[38;5;241m=\u001b[39mhas_token, kwargs\u001b[38;5;241m=\u001b[39mkwargs)\n\u001b[0;32m--> 119\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/mgie/lib/python3.10/site-packages/huggingface_hub/file_download.py:1674\u001b[0m, in \u001b[0;36mget_hf_file_metadata\u001b[0;34m(url, token, proxies, timeout, library_name, library_version, user_agent, headers)\u001b[0m\n\u001b[1;32m   1673\u001b[0m \u001b[38;5;66;03m# Retrieve metadata\u001b[39;00m\n\u001b[0;32m-> 1674\u001b[0m r \u001b[38;5;241m=\u001b[39m \u001b[43m_request_wrapper\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1675\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mHEAD\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1676\u001b[0m \u001b[43m    \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1677\u001b[0m \u001b[43m    \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1678\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_redirects\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   1679\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfollow_relative_redirects\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   1680\u001b[0m \u001b[43m    \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1681\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1682\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1683\u001b[0m hf_raise_for_status(r)\n",
      "File \u001b[0;32m~/miniconda3/envs/mgie/lib/python3.10/site-packages/huggingface_hub/file_download.py:369\u001b[0m, in \u001b[0;36m_request_wrapper\u001b[0;34m(method, url, follow_relative_redirects, **params)\u001b[0m\n\u001b[1;32m    368\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m follow_relative_redirects:\n\u001b[0;32m--> 369\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43m_request_wrapper\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    370\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    371\u001b[0m \u001b[43m        \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    372\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfollow_relative_redirects\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    373\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    374\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    376\u001b[0m     \u001b[38;5;66;03m# If redirection, we redirect only relative paths.\u001b[39;00m\n\u001b[1;32m    377\u001b[0m     \u001b[38;5;66;03m# This is useful in case of a renamed repository.\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/mgie/lib/python3.10/site-packages/huggingface_hub/file_download.py:392\u001b[0m, in \u001b[0;36m_request_wrapper\u001b[0;34m(method, url, follow_relative_redirects, **params)\u001b[0m\n\u001b[1;32m    391\u001b[0m \u001b[38;5;66;03m# Perform request and return if status_code is not in the retry list.\u001b[39;00m\n\u001b[0;32m--> 392\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[43mget_session\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    393\u001b[0m hf_raise_for_status(response)\n",
      "File \u001b[0;32m~/miniconda3/envs/mgie/lib/python3.10/site-packages/requests/sessions.py:589\u001b[0m, in \u001b[0;36mSession.request\u001b[0;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[1;32m    588\u001b[0m send_kwargs\u001b[38;5;241m.\u001b[39mupdate(settings)\n\u001b[0;32m--> 589\u001b[0m resp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprep\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43msend_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    591\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m resp\n",
      "File \u001b[0;32m~/miniconda3/envs/mgie/lib/python3.10/site-packages/requests/sessions.py:703\u001b[0m, in \u001b[0;36mSession.send\u001b[0;34m(self, request, **kwargs)\u001b[0m\n\u001b[1;32m    702\u001b[0m \u001b[38;5;66;03m# Send the request\u001b[39;00m\n\u001b[0;32m--> 703\u001b[0m r \u001b[38;5;241m=\u001b[39m \u001b[43madapter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    705\u001b[0m \u001b[38;5;66;03m# Total elapsed time of the request (approximately)\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/mgie/lib/python3.10/site-packages/huggingface_hub/utils/_http.py:68\u001b[0m, in \u001b[0;36mUniqueRequestIdAdapter.send\u001b[0;34m(self, request, *args, **kwargs)\u001b[0m\n\u001b[1;32m     67\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 68\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     69\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m requests\u001b[38;5;241m.\u001b[39mRequestException \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m~/miniconda3/envs/mgie/lib/python3.10/site-packages/requests/adapters.py:507\u001b[0m, in \u001b[0;36mHTTPAdapter.send\u001b[0;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[1;32m    506\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(e\u001b[38;5;241m.\u001b[39mreason, NewConnectionError):\n\u001b[0;32m--> 507\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m ConnectTimeout(e, request\u001b[38;5;241m=\u001b[39mrequest)\n\u001b[1;32m    509\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(e\u001b[38;5;241m.\u001b[39mreason, ResponseError):\n",
      "\u001b[0;31mConnectTimeout\u001b[0m: (MaxRetryError(\"HTTPSConnectionPool(host='huggingface.co', port=443): Max retries exceeded with url: /jordiclive/flan-t5-11b-summarizer-filtered/resolve/main/config.json (Caused by ConnectTimeoutError(<urllib3.connection.HTTPSConnection object at 0x7fa7a8e56200>, 'Connection to huggingface.co timed out. (connect timeout=10)'))\"), '(Request ID: 25fa912e-4c66-4f80-8100-04a41b635e3e)')",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mLocalEntryNotFoundError\u001b[0m                   Traceback (most recent call last)",
      "File \u001b[0;32m~/miniconda3/envs/mgie/lib/python3.10/site-packages/transformers/utils/hub.py:409\u001b[0m, in \u001b[0;36mcached_file\u001b[0;34m(path_or_repo_id, filename, cache_dir, force_download, resume_download, proxies, use_auth_token, revision, local_files_only, subfolder, user_agent, _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors, _commit_hash)\u001b[0m\n\u001b[1;32m    407\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    408\u001b[0m     \u001b[38;5;66;03m# Load from URL or cache if already cached\u001b[39;00m\n\u001b[0;32m--> 409\u001b[0m     resolved_file \u001b[38;5;241m=\u001b[39m \u001b[43mhf_hub_download\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    410\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpath_or_repo_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    411\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    412\u001b[0m \u001b[43m        \u001b[49m\u001b[43msubfolder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43msubfolder\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43msubfolder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    413\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrevision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    414\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    415\u001b[0m \u001b[43m        \u001b[49m\u001b[43muser_agent\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muser_agent\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    416\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    417\u001b[0m \u001b[43m        \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    418\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    419\u001b[0m \u001b[43m        \u001b[49m\u001b[43muse_auth_token\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_auth_token\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    420\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    421\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    423\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m RepositoryNotFoundError:\n",
      "File \u001b[0;32m~/miniconda3/envs/mgie/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py:119\u001b[0m, in \u001b[0;36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    117\u001b[0m     kwargs \u001b[38;5;241m=\u001b[39m smoothly_deprecate_use_auth_token(fn_name\u001b[38;5;241m=\u001b[39mfn\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, has_token\u001b[38;5;241m=\u001b[39mhas_token, kwargs\u001b[38;5;241m=\u001b[39mkwargs)\n\u001b[0;32m--> 119\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/mgie/lib/python3.10/site-packages/huggingface_hub/file_download.py:1406\u001b[0m, in \u001b[0;36mhf_hub_download\u001b[0;34m(repo_id, filename, subfolder, repo_type, revision, library_name, library_version, cache_dir, local_dir, local_dir_use_symlinks, user_agent, force_download, force_filename, proxies, etag_timeout, resume_download, token, local_files_only, headers, legacy_cache_layout, endpoint)\u001b[0m\n\u001b[1;32m   1404\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1405\u001b[0m         \u001b[38;5;66;03m# Otherwise: most likely a connection issue or Hub downtime => let's warn the user\u001b[39;00m\n\u001b[0;32m-> 1406\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m LocalEntryNotFoundError(\n\u001b[1;32m   1407\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error happened while trying to locate the file on the Hub and we cannot find the requested files\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1408\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m in the local cache. Please check your connection and try again or make sure your Internet connection\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1409\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m is on.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1410\u001b[0m         ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mhead_call_error\u001b[39;00m\n\u001b[1;32m   1412\u001b[0m \u001b[38;5;66;03m# From now on, etag and commit_hash are not None.\u001b[39;00m\n",
      "\u001b[0;31mLocalEntryNotFoundError\u001b[0m: An error happened while trying to locate the file on the Hub and we cannot find the requested files in the local cache. Please check your connection and try again or make sure your Internet connection is on.",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m get_ipython()\u001b[38;5;241m.\u001b[39msystem(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msource /etc/network_turbo\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m----> 2\u001b[0m summer \u001b[38;5;241m=\u001b[39m \u001b[43mtransformers\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpipeline\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43msummarization\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mjordiclive/flan-t5-11b-summarizer-filtered\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtorch_dtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mT\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbfloat16\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/mgie/lib/python3.10/site-packages/transformers/pipelines/__init__.py:692\u001b[0m, in \u001b[0;36mpipeline\u001b[0;34m(task, model, config, tokenizer, feature_extractor, image_processor, framework, revision, use_fast, use_auth_token, device, device_map, torch_dtype, trust_remote_code, model_kwargs, pipeline_class, **kwargs)\u001b[0m\n\u001b[1;32m    690\u001b[0m     hub_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_commit_hash\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m config\u001b[38;5;241m.\u001b[39m_commit_hash\n\u001b[1;32m    691\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m config \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(model, \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m--> 692\u001b[0m     config \u001b[38;5;241m=\u001b[39m \u001b[43mAutoConfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_from_pipeline\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mhub_kwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    693\u001b[0m     hub_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_commit_hash\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m config\u001b[38;5;241m.\u001b[39m_commit_hash\n\u001b[1;32m    695\u001b[0m custom_tasks \u001b[38;5;241m=\u001b[39m {}\n",
      "File \u001b[0;32m~/miniconda3/envs/mgie/lib/python3.10/site-packages/transformers/models/auto/configuration_auto.py:905\u001b[0m, in \u001b[0;36mAutoConfig.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[0m\n\u001b[1;32m    903\u001b[0m kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mname_or_path\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m pretrained_model_name_or_path\n\u001b[1;32m    904\u001b[0m trust_remote_code \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrust_remote_code\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m--> 905\u001b[0m config_dict, unused_kwargs \u001b[38;5;241m=\u001b[39m \u001b[43mPretrainedConfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_config_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    906\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mauto_map\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m config_dict \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAutoConfig\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m config_dict[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mauto_map\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n\u001b[1;32m    907\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m trust_remote_code:\n",
      "File \u001b[0;32m~/miniconda3/envs/mgie/lib/python3.10/site-packages/transformers/configuration_utils.py:573\u001b[0m, in \u001b[0;36mPretrainedConfig.get_config_dict\u001b[0;34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[0m\n\u001b[1;32m    571\u001b[0m original_kwargs \u001b[38;5;241m=\u001b[39m copy\u001b[38;5;241m.\u001b[39mdeepcopy(kwargs)\n\u001b[1;32m    572\u001b[0m \u001b[38;5;66;03m# Get config dict associated with the base config file\u001b[39;00m\n\u001b[0;32m--> 573\u001b[0m config_dict, kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_config_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    574\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_commit_hash\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m config_dict:\n\u001b[1;32m    575\u001b[0m     original_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_commit_hash\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m config_dict[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_commit_hash\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[0;32m~/miniconda3/envs/mgie/lib/python3.10/site-packages/transformers/configuration_utils.py:628\u001b[0m, in \u001b[0;36mPretrainedConfig._get_config_dict\u001b[0;34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[0m\n\u001b[1;32m    624\u001b[0m configuration_file \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_configuration_file\u001b[39m\u001b[38;5;124m\"\u001b[39m, CONFIG_NAME)\n\u001b[1;32m    626\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    627\u001b[0m     \u001b[38;5;66;03m# Load from local folder or from cache or download from model Hub and cache\u001b[39;00m\n\u001b[0;32m--> 628\u001b[0m     resolved_config_file \u001b[38;5;241m=\u001b[39m \u001b[43mcached_file\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    629\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    630\u001b[0m \u001b[43m        \u001b[49m\u001b[43mconfiguration_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    631\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    632\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    633\u001b[0m \u001b[43m        \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    634\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    635\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    636\u001b[0m \u001b[43m        \u001b[49m\u001b[43muse_auth_token\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_auth_token\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    637\u001b[0m \u001b[43m        \u001b[49m\u001b[43muser_agent\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muser_agent\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    638\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrevision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    639\u001b[0m \u001b[43m        \u001b[49m\u001b[43msubfolder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msubfolder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    640\u001b[0m \u001b[43m        \u001b[49m\u001b[43m_commit_hash\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcommit_hash\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    641\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    642\u001b[0m     commit_hash \u001b[38;5;241m=\u001b[39m extract_commit_hash(resolved_config_file, commit_hash)\n\u001b[1;32m    643\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mEnvironmentError\u001b[39;00m:\n\u001b[1;32m    644\u001b[0m     \u001b[38;5;66;03m# Raise any environment error raise by `cached_file`. It will have a helpful error message adapted to\u001b[39;00m\n\u001b[1;32m    645\u001b[0m     \u001b[38;5;66;03m# the original exception.\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/mgie/lib/python3.10/site-packages/transformers/utils/hub.py:443\u001b[0m, in \u001b[0;36mcached_file\u001b[0;34m(path_or_repo_id, filename, cache_dir, force_download, resume_download, proxies, use_auth_token, revision, local_files_only, subfolder, user_agent, _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors, _commit_hash)\u001b[0m\n\u001b[1;32m    441\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m _raise_exceptions_for_missing_entries \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m _raise_exceptions_for_connection_errors:\n\u001b[1;32m    442\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 443\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mEnvironmentError\u001b[39;00m(\n\u001b[1;32m    444\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWe couldn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt connect to \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mHUGGINGFACE_CO_RESOLVE_ENDPOINT\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m to load this file, couldn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt find it in the\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    445\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m cached files and it looks like \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpath_or_repo_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m is not the path to a directory containing a file named\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    446\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfull_filename\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mCheckout your internet connection or see how to run the library in offline mode at\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    447\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhttps://huggingface.co/docs/transformers/installation#offline-mode\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    448\u001b[0m     )\n\u001b[1;32m    449\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m EntryNotFoundError:\n\u001b[1;32m    450\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m _raise_exceptions_for_missing_entries:\n",
      "\u001b[0;31mOSError\u001b[0m: We couldn't connect to 'https://huggingface.co' to load this file, couldn't find it in the cached files and it looks like jordiclive/flan-t5-11b-summarizer-filtered is not the path to a directory containing a file named config.json.\nCheckout your internet connection or see how to run the library in offline mode at 'https://huggingface.co/docs/transformers/installation#offline-mode'."
     ]
    }
   ],
   "source": [
    "!source /etc/network_turbo\n",
    "summer = transformers.pipeline('summarization', 'jordiclive/flan-t5-11b-summarizer-filtered', torch_dtype=T.bfloat16, device=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25ac2e16-32a8-45ea-acc9-37b28a0a9681",
   "metadata": {},
   "outputs": [],
   "source": [
    "pkl, tsv, ei = {'task': []}, open('./_data/ipr2pr.tsv', 'w'), {}\n",
    "\n",
    "lst = glob('_data/*/prompt.json')\n",
    "for file in tqdm(lst):\n",
    "    prompt = json.load(open(file, 'r'))\n",
    "    txt = prompt['edit']\n",
    "\n",
    "    txt = \"what will this image be like if '%s'  (in a short paragraph)\"%(txt)\n",
    "    txt = txt+'\\n'+DEFAULT_IM_START_TOKEN+DEFAULT_IMAGE_PATCH_TOKEN*image_token_len+DEFAULT_IM_END_TOKEN\n",
    "    conv = conv_templates['vicuna_v1_1'].copy()\n",
    "    conv.append_message(conv.roles[0], txt), conv.append_message(conv.roles[1], None)\n",
    "    txt = conv.get_prompt()\n",
    "    txt = tokenizer(txt)\n",
    "    txt, mask = T.as_tensor(txt['input_ids']), T.as_tensor(txt['attention_mask'])\n",
    "    \n",
    "    for img in glob('/'.join(file.split('/')[:-1])+'/*_0.jpg'):\n",
    "        item = file.split('/')[-2]+'_'+img.split('/')[-1].replace('.jpg', '')\n",
    "        inp, ans = Image.open(img).convert('RGB'), Image.open(img.replace('_0.jpg', '_1.jpg')).convert('RGB')\n",
    "        \n",
    "        img = image_processor.preprocess(inp, return_tensors='pt')['pixel_values'][0]\n",
    "        with T.inference_mode():\n",
    "            out = model.generate(txt.unsqueeze(dim=0).cuda(), images=img.half().unsqueeze(dim=0).cuda(), attention_mask=mask.unsqueeze(dim=0).cuda(), \n",
    "                                 do_sample=False, max_new_tokens=1024)[0].tolist()\n",
    "            \n",
    "            out = remove_alter(tokenizer.decode(out))\n",
    "            res = summer(['summarize the following paragraph in 32 words:\\n\\n%s'%(out)], num_beams=5, min_length=5, max_length=64, \n",
    "                         do_sample=False, no_repeat_ngram_size=3, truncation=True)[0]['summary_text']\n",
    "\n",
    "        pkl['task'].append([{'input': item, 'answer': item.replace('_0', '_1'), 'instruction': prompt['edit'], 'lineidx': tsv.tell()}])\n",
    "        tsv.write('%s\\t%s\\n'%(f2b(inp), f2b(ans)))\n",
    "        ei[item] = {'instruction': prompt['edit'], 'expressive': res}\n",
    "\n",
    "pickle.dump(pkl, open('./_data/ipr2pr.pkl', 'wb'))\n",
    "tsv.flush(), tsv.close()\n",
    "json.dump(ei, open('./_data/ipr2pr_expressive.json', 'w'), indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c36ea959-ff15-4ebd-848f-13de020886a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "pkl, tsv, ei = pickle.load(open('./_data/ipr2pr.pkl', 'rb')), open('./_data/ipr2pr.tsv', 'r'), json.load(open('./_data/ipr2pr_expressive.json', 'r'))\n",
    "for task in pkl['task']:\n",
    "    task = task[0]\n",
    "    tsv.seek(task['lineidx'])\n",
    "    b = tsv.readline().strip().split('\\t')\n",
    "    print(task)\n",
    "    display(b2f(b[0])), display(b2f(b[1]))\n",
    "    print(ei[task['input']])\n",
    "    print('\\n-----\\n')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mgie",
   "language": "python",
   "name": "mgie"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
